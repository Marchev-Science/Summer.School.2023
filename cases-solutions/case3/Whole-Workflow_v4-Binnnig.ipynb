{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5d71fbd",
   "metadata": {},
   "source": [
    "# TODOs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5445581f",
   "metadata": {},
   "source": [
    "1. Data Generation:\n",
    "    - numerical data generation - improvements in distribution ***DONE***\n",
    "    - business rules adjustments (only after numerical data generation is fixed)\n",
    "    - check the distribution of each numerical feature of the merged data ***DONE***\n",
    "    - categorical data - add a check (with Kolmogorov Smirnov or another one) that tells you if the distribution of the synthetic data is ok - ***DONE***\n",
    "    - Marchev added pers_exp as a categorical column; it also exists in the numerical cols. Our task is to first generate the distribution (and values) for this column (assuming normal distribution) and based on it generate the rest.\n",
    "<br>\n",
    "<br>\n",
    "2. Feature Engineering:\n",
    "    - Generate new features (**custom made**, statistical) - ***Girls***\n",
    "    - Balance data \n",
    "        - research other methods beside SMOTE; **Multiclass problems handling - should we resample for each target class individually or not. Describe all possible prediction scenarious** - ***ALL***\n",
    "<br>\n",
    "<br>\n",
    "3. Feature Selection \n",
    "    - research methods for numerical and categorical feature selections\n",
    "<br>\n",
    "<br>\n",
    "4. Modeling:\n",
    "    - try different models - with at least 5k examples!!! \n",
    "        - black box and explainable ones\n",
    "    - visualizations - compare performances\n",
    "    - try to explain the black box models (OPTIONAL)\n",
    "    - hyperparam optimization (OPTIONAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553b783b",
   "metadata": {},
   "source": [
    "# Inputs and tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd05fea7",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9844e263",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T08:46:27.331988Z",
     "start_time": "2023-07-23T08:46:24.586792Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix,roc_curve, roc_auc_score, precision_score, recall_score, precision_recall_curve\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "\n",
    "# from fitter import Fitter, get_common_distributions, get_distributions\n",
    "import random\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Increase the maximum number of rows and columns to be displayed\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.max_columns', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2e4965",
   "metadata": {},
   "source": [
    "## Categorical data distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e973056b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T08:46:28.466306Z",
     "start_time": "2023-07-23T08:46:28.427266Z"
    }
   },
   "outputs": [],
   "source": [
    "dists = {\n",
    "#     'pers_exp':{'labels':['group1', 'group2', 'group3', 'group4'], 'values':[0.10,0.44,0.33,0.14]},\n",
    "    'sex':{'labels':['M', 'F'], 'values':[0.4854368932,0.5145631068]},\n",
    "    'lv_educ':{'labels':['Incomplete', 'Primary', 'Basic', 'Secondary', 'Higher'], 'values':[0.0595,0.07788016474,0.2309254283,0.4359496722,0.1957067711]},\n",
    "    'empl_stat':{'labels':['Employers', 'Self-employed', 'Employed in private sector', 'Employed in public sector', 'Unpaid family workers'], 'values':[0.03631598652,0.07272557095,0.6708785723,0.2126544365,0.00742543367]},\n",
    "    'marit_stat':{'labels':['Single', 'Married', 'Divorced', 'Widowed'], 'values':[0.397,0.443,0.058,0.102]},\n",
    "    'house_memb':{'labels':['1', '2', '3', '4', '5', '6', '7+'], 'values':[0.1805,0.3778,0.2387,0.1157,0.0525,0.0238,0.011]},\n",
    "    'chil_u_18_y':{'labels':['No children under 18', 'One child under 18', 'Two children under 18', 'Three children under 18', 'Four children under 18', 'Five children under 18', 'Six or more children under 18'], 'values':[0.422602157,0.36552047,0.183222339,0.020674764,0.004993779,0.001875149,0.001111341]},\n",
    "    'nation':{'labels':['Bulgaria', 'EU', 'Other'], 'values':[0.9950198043,0.001146570676,0.003833625045]},\n",
    "    'religion':{'labels':['Protestant', 'Catholic', 'Orthodox', 'Muslim', 'Other', 'No religion', 'I do not identify myself'], 'values':[0.011,0.008,0.76,0.1,0.002,0.047,0.072]},\n",
    "    'soc_econ_stat':{'labels':['Economically active', 'Economically inactive'], 'values':[0.6151643031,0.3848356969]},\n",
    "    'prof_ind':{'labels':['Agriculture, forestry and fisheries', 'Mining and processing industry', 'Utilities (electricity distribution and water supply)', 'Construction', 'Trade, automobile and motorcycle repair', 'Transportation, warehousing and mail', 'Hospitality and restaurant services', 'Creation and distribution of information and creative products, Telecommunications', 'Financial and administrative activities', 'Public administration', 'Education and research', 'Human health and social work', 'Other activities'], 'values':[0.03090815115,0.2353,0.029,0.05523651408,0.1645618594,0.06439111505,0.05161626582,0.03936261795,0.07356911161,0.04836124844,0.104946474,0.06006423384,0.04269692032]},\n",
    "    'prof_stat':{'labels':['Management contract', 'Employment contract', 'Civil contract', 'Self-employed', 'Unemployed', 'Pensioner'], 'values':[0.01783393631,0.4732428049,0.02497602302,0.0385148509,0.167699009,0.277733376]},\n",
    "    'count_house':{'labels':['0', '1', '2+'], 'values':[0.37,0.6,0.03]},\n",
    "    'own_field':{'labels':['YES', 'NO'], 'values':[0.2621335023,0.737866497676384]},\n",
    "    'num_car_house':{'labels':['0', '1', '2', '3+'], 'values':[0.5714285714,0.36,0.06428571429,0.004285714286]},\n",
    "    'own_rent_house':{'labels':['my own', 'rented'], 'values':[0.843,0.157]},\n",
    "    'edu':{'labels':['Educational Sciences', 'Humanities', 'Social, Economic and Legal Sciences', 'Natural Sciences, Mathematics and Informatics', 'Technical Sciences', 'Agricultural Sciences and Veterinary Medicine', 'Health and Sports', 'Arts', 'Security and Defense'], 'values':[0.07591254907,0.0461889827,0.5266633332,0.04571641724,0.1533297557,0.01776640163,0.0930038303,0.02247374859,0.01891291637]},\n",
    "    'temp':{'labels':['Choleric', 'Phlegmatic', 'Sanguine', 'Melancholic'], 'values':[0.38,0.11,0.23,0.28]},\n",
    "    'invest_exp':{'labels':['0', '1-5', '6-10', '11-15', '16-25'], 'values':[0.7,0.2,0.06,0.03,0.01]},\n",
    "    'shares':{'labels':['YES', 'NO'], 'values':[0.003394353314,0.9966056467]},\n",
    "    'corp_oblig':{'labels':['YES', 'NO'], 'values':[0.0003792213936,0.9996207786]},\n",
    "    'oth':{'labels':['YES', 'NO'], 'values':[0.000592597502012084,0.999407402497988]},\n",
    "    'inv_fund':{'labels':['YES', 'NO'], 'values':[0.06491199709,0.9350880029]},\n",
    "    'cash':{'labels':['YES', 'NO'], 'values':[0.04105169923,0.9589483008]},\n",
    "    'crypto':{'labels':['YES', 'NO'], 'values':[0.003284135938,0.9967158641]},\n",
    "    'gov_bond':{'labels':['YES', 'NO'], 'values':[0.06835666691,0.9316433331]},\n",
    "    'deposits':{'labels':['YES', 'NO'], 'values':[0.8180293286,0.1819706714]},\n",
    "    'banking':{'labels':['Online', 'Offline'], 'values':[0.09,0.91]},\n",
    "    'bk_oprat':{'labels':['Up to 7', 'From 8 to 10', 'From 11 to 13', 'From 14 to 18', 'From 19 to more'], 'values':[0.0084,0.2424,0.4729,0.2615,0.0148]},\n",
    "    'bk_dc':{'labels':['Under one', 'One', 'Two', 'Three'], 'values':[0.01,0.57,0.38,0.04]},\n",
    "    'bk_cc':{'labels':['YES', 'NO'], 'values':[0.17,0.83]},\n",
    "    'bk_acc':{'labels':['YES', 'NO'], 'values':[0.8634087377,0.1365912623]},\n",
    "    'ins_prop':{'labels':['YES', 'NO'], 'values':[0.05,0.95]},\n",
    "    'ins_life':{'labels':['YES', 'NO'], 'values':[0.09,0.91]},\n",
    "    'ins_casco':{'labels':['YES', 'NO'], 'values':[0.03,0.97]},\n",
    "    'health_ins':{'labels':['YES', 'NO'], 'values':[0.02,0.98]},\n",
    "    'overdraft':{'labels':['YES', 'NO'], 'values':[0.19,0.81]},\n",
    "    'cons_cred':{'labels':['YES', 'NO'], 'values':[0.26,0.74]},\n",
    "    'mortgage':{'labels':['YES', 'NO'], 'values':[0.02,0.98]},\n",
    "    'car_leas':{'labels':['YES', 'NO'], 'values':[0.2,0.8]},\n",
    "    'pens_ins':{'labels':['YES', 'NO'], 'values':[0.11,0.89]},\n",
    "    'overdraft_app':{'labels':['YES', 'NO'], 'values':[0.2439,0.7561]},\n",
    "    'cons_cred_app':{'labels':['YES', 'NO'], 'values':[0.305299502487562,0.694700497512438]},\n",
    "    'mortgage_app':{'labels':['YES', 'NO'], 'values':[0.03,0.97]},\n",
    "    'bk_cc_app':{'labels':['YES', 'NO'], 'values':[0.21,0.79]}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637a4dbe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T08:46:28.987989Z",
     "start_time": "2023-07-23T08:46:28.973638Z"
    }
   },
   "outputs": [],
   "source": [
    "len(dists)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01e3c2f",
   "metadata": {},
   "source": [
    "## Correlation of Numerical data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f1a5da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T08:46:29.697197Z",
     "start_time": "2023-07-23T08:46:29.682053Z"
    }
   },
   "outputs": [],
   "source": [
    "corr = {\n",
    "    'features': ['age', 'ind_risk', 'income', 'pers_exp', 'house_exp', 'taxes', 'transp_telecom', 'hobby'],\n",
    "    'age': [1, -0.00665947056405372, 0.00291644965339247, 0.0107779942638097, 0.00698674581731255, 0.00729153655132963, 0.0099866509330216, 0.00931630696561133],\n",
    "    'ind_risk': [-0.00665947056405372, 1, 0.0039918072709289, 0.00806259039194059, 0.00457023635440603, 0.0061985340641631, 0.00768699810849585, -0.00332322616613201],\n",
    "    'income': [0.00291644965339247, 0.0039918072709289, 1, 0.560949334881676, 0.58892666343229, 0.581907424628933, 0.562946509689962, 0.352350802339294],\n",
    "    'pers_exp': [0.0107779942638097, 0.00806259039194059, 0.560949334881676, 1, 0.928449923861951, 0.929598634668897, 0.934775947642248, 0.714298364869941],\n",
    "    'house_exp': [0.00698674581731255, 0.00457023635440603, 0.58892666343229, 0.928449923861951, 1, 0.93031279279417, 0.927846735467478, 0.679286362990223],\n",
    "    'taxes': [0.00729153655132963, 0.0061985340641631, 0.581907424628933, 0.929598634668897, 0.93031279279417, 1, 0.92920510128812, 0.689442053350162],\n",
    "    'transp_telecom': [0.0099866509330216, 0.00768699810849585, 0.562946509689962, 0.934775947642248, 0.927846735467478, 0.92920510128812, 1, 0.714114127908189],\n",
    "    'hobby': [0.00931630696561133, -0.00332322616613201, 0.352350802339294, 0.714298364869941, 0.679286362990223, 0.689442053350162, 0.714114127908189, 1]\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "means = [50.58, 0.6039, 13579.30, 5305.00, 2260, 1500, 1335, 1740]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11400b94",
   "metadata": {},
   "source": [
    "## Extract distributions from dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c072733",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T08:46:30.456080Z",
     "start_time": "2023-07-23T08:46:30.440944Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_dists(x, dists):\n",
    "    '''\n",
    "    A function to extract distributions from the dictionary dists,where:\n",
    "    x is the name of the feature to extract in ''\n",
    "    dists is the dictionary with all distributions\n",
    "    '''\n",
    "    import pandas as pd\n",
    "    column_names = dists[x]['labels']\n",
    "    values = [dists[x]['values']]\n",
    "    pd_df = pd.DataFrame(data=values, columns=column_names)\n",
    "    pd_df.index = pd.Index([x])\n",
    "    return pd_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e2cd86",
   "metadata": {},
   "source": [
    "## Convert the dictionary with correlation matrix to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2d6b3d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T08:46:31.463204Z",
     "start_time": "2023-07-23T08:46:31.456204Z"
    }
   },
   "outputs": [],
   "source": [
    "def corr2df(corr):\n",
    "    '''\n",
    "    A function to create correlation dataframe from dictionary corr, where\n",
    "    corr is the dictionary with the correlation matrix\n",
    "    '''\n",
    "    import pandas as df\n",
    "    corr_df = pd.DataFrame(corr)\n",
    "    corr_df.set_index('features', inplace=True)\n",
    "    corr_df.index.name=None\n",
    "    return corr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5906342",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7e2a38d8",
   "metadata": {},
   "source": [
    "# Data Generation (Data synthesis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b5e8bf",
   "metadata": {},
   "source": [
    "## Categorical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645787fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T08:46:33.782721Z",
     "start_time": "2023-07-23T08:46:33.203678Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Number of rows in the synthetic dataset\n",
    "num_rows = 250000\n",
    "\n",
    "# Create the synthetic dataset\n",
    "dataset = {}\n",
    "for key, value_dict in dists.items():\n",
    "    labels = value_dict['labels']\n",
    "    probabilities = value_dict['values']\n",
    "    \n",
    "    # Normalize probabilities to ensure they sum to 1\n",
    "    normalized_probabilities = probabilities / np.sum(probabilities)\n",
    "    \n",
    "    sampled_values = np.random.choice(labels, size=num_rows, p=normalized_probabilities)\n",
    "    dataset[key] = sampled_values\n",
    "\n",
    "# Printing the first 10 rows of the synthetic dataset\n",
    "# for key in dataset:\n",
    "#     print(f\"{key}: {dataset[key][:10]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f269b8fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T08:46:34.970218Z",
     "start_time": "2023-07-23T08:46:34.004401Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "cat_df = pd.DataFrame(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2344c92f",
   "metadata": {},
   "source": [
    "### Perform a Kolmogorov-Smirnov distribution test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2200934",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T08:46:35.130218Z",
     "start_time": "2023-07-23T08:46:35.116221Z"
    }
   },
   "outputs": [],
   "source": [
    "def kolmogorov_smirnov_test(categorical_data, old_distributions):\n",
    "    \n",
    "    from scipy.stats import ks_2samp\n",
    "    \n",
    "#     old = {'sex': {'labels': ['M', 'F'], 'values': [0.4854368932, 0.5145631068]}}\n",
    "#     new = {'sex': {'labels': ['M', 'F'], 'values': [0.476, 0.524]}}\n",
    "\n",
    "    statistics = {}\n",
    "    \n",
    "    for col in categorical_data:\n",
    "        \n",
    "        print()\n",
    "        print(\"Reviewed column: \", col)\n",
    "        # Extract the values for each category in the old and new distributions\n",
    "        \n",
    "        \n",
    "        old_values = old_distributions[col]['values']\n",
    "        old_labels = old_distributions[col]['labels']\n",
    "        \n",
    "        new_data_labels = list(categorical_data[col].value_counts(normalize=True).index)\n",
    "        \n",
    "        old_dis_data = []\n",
    "        new_dis_data = []\n",
    "        \n",
    "        \n",
    "        for i in range(len(old_labels)):\n",
    "            \n",
    "            label = old_labels[i]\n",
    "            old_dis_data.append(old_distributions[col]['values'][i])\n",
    "            \n",
    "            new_data_value = categorical_data[col].value_counts(normalize=True)[label]\n",
    "            new_dis_data.append(new_data_value)\n",
    "        \n",
    "        old_dis_data_array = np.array(old_dis_data)\n",
    "        new_dis_data_array = np.array(new_dis_data)\n",
    "        \n",
    "        print(f\"old dist: {old_dis_data_array}\")\n",
    "        print(f\"new dist: {new_dis_data_array}\")\n",
    "        # Perform the KS test\n",
    "        ks_statistic, p_value = ks_2samp(old_dis_data_array, new_dis_data_array)\n",
    "\n",
    "        # Define the significance level (alpha) to test against the p-value\n",
    "        alpha = 0.05\n",
    "\n",
    "        # Check if the p-value is less than the significance level\n",
    "        \n",
    "        \n",
    "        if p_value < alpha:\n",
    "            print(f\"Distributions are different for column: {col}, p_value={1-p_value}\")\n",
    "            \n",
    "            statistics[col] = True\n",
    "        else:\n",
    "            print(f\"Distributions are similar for column: {col}, p_value={1-p_value}\")\n",
    "            \n",
    "            statistics[col] = False\n",
    "    \n",
    "    print(\"......................\")\n",
    "    print()\n",
    "    return statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0d43e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T08:46:40.621986Z",
     "start_time": "2023-07-23T08:46:35.716576Z"
    }
   },
   "outputs": [],
   "source": [
    "categorical_ks_statistics = kolmogorov_smirnov_test(categorical_data=cat_df,\n",
    "                                                   old_distributions=dists)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5025252d",
   "metadata": {},
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e7ed35",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T08:46:40.733307Z",
     "start_time": "2023-07-23T08:46:40.720979Z"
    }
   },
   "outputs": [],
   "source": [
    "# cat_df.to_csv(\"df_cat_No_Br_250k_v2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc8bf26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc77d202",
   "metadata": {},
   "source": [
    "Notes: \n",
    "- generate all cat data (including pers_exp\n",
    "- use it's values as a basis when generating the numerical data (all other features in the corr matrix should be generated based on their relationship with the pers_exp) and the MEANS provided in the inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f14c15d",
   "metadata": {},
   "source": [
    "## Numerical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3e9bb1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T08:46:42.407404Z",
     "start_time": "2023-07-23T08:46:42.386642Z"
    }
   },
   "outputs": [],
   "source": [
    "corr.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e4dd5a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T08:46:42.630436Z",
     "start_time": "2023-07-23T08:46:42.618694Z"
    }
   },
   "outputs": [],
   "source": [
    "l_corr_matrix = []\n",
    "\n",
    "for k, v in corr.items():\n",
    "    if k == \"features\":\n",
    "        continue\n",
    "    else:\n",
    "        l_corr_matrix.append(v)\n",
    "\n",
    "        \n",
    "arr_corr_matrix = np.array(l_corr_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1eac74",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T08:46:42.852778Z",
     "start_time": "2023-07-23T08:46:42.840326Z"
    }
   },
   "outputs": [],
   "source": [
    "print(arr_corr_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762f63f7",
   "metadata": {},
   "source": [
    "### Generate synthetic data (Copula)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d03e128",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T08:46:46.782634Z",
     "start_time": "2023-07-23T08:46:46.321387Z"
    }
   },
   "outputs": [],
   "source": [
    "feature_bounds = []\n",
    "\n",
    "possible_values = {\n",
    "    'age': [20, 86],\n",
    "    'ind_risk': [0, 1],\n",
    "    'income': [0, 150000],\n",
    "    'pers_exp': [0, 6000],\n",
    "    'house_exp': [0, 4000],\n",
    "    'taxes': [0, 2500],\n",
    "    'transp_telecom': [0, 2500],\n",
    "    'hobby': [0, 3000],\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "for v in possible_values.values():\n",
    "    \n",
    "    feature_bounds.append(tuple(v))\n",
    "\n",
    "        \n",
    "# mean = []\n",
    "\n",
    "# for k, v in possible_values.items():\n",
    "#     avg = sum(v)/len(v)\n",
    "#     mean.append(avg)  \n",
    "\n",
    "    \n",
    "# std = [int(v/3) for v in mean]\n",
    "std = [3]*8\n",
    "    \n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def generate_synthetic_data_with_bounds(correlation_matrix, num_samples, feature_bounds, means=means, std=std):\n",
    "    num_features = correlation_matrix.shape[0]\n",
    "    lower_bounds, upper_bounds = zip(*feature_bounds)\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    \n",
    "    # Check if the correlation matrix is valid (symmetric and positive definite)\n",
    "    if not np.allclose(correlation_matrix, correlation_matrix.T):\n",
    "        raise ValueError(\"Correlation matrix must be symmetric.\")\n",
    "    if not np.all(np.linalg.eigvals(correlation_matrix) > 0):\n",
    "        raise ValueError(\"Correlation matrix must be positive definite.\")\n",
    "    \n",
    "    # Generate synthetic data using multivariate normal distribution\n",
    "    mean = np.zeros(num_features)\n",
    "    synthetic_data = np.random.multivariate_normal(mean, correlation_matrix, num_samples)\n",
    "    \n",
    "    # Apply Gaussian copula to maintain correlation structure\n",
    "    synthetic_data = norm.cdf(synthetic_data, loc=mean, scale=std)\n",
    "    \n",
    "    # Scale the data to the specified bounds for each feature\n",
    "    for i in range(num_features):\n",
    "        synthetic_data[:, i] = lower_bounds[i] + synthetic_data[:, i] * (upper_bounds[i] - lower_bounds[i])\n",
    "    \n",
    "    return synthetic_data\n",
    "\n",
    "# Example usage:\n",
    "correlation_matrix = np.array(l_corr_matrix)\n",
    "\n",
    "num_samples = 250000\n",
    "\n",
    "synthetic_data = generate_synthetic_data_with_bounds(correlation_matrix, num_samples, feature_bounds)\n",
    "print(synthetic_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d21d85",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T08:46:47.361419Z",
     "start_time": "2023-07-23T08:46:47.292631Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "adjusted_df = pd.DataFrame(synthetic_data, columns=possible_values.keys())\n",
    "adjusted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f30620",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T08:46:48.037228Z",
     "start_time": "2023-07-23T08:46:48.011103Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(correlation_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f5f99f",
   "metadata": {},
   "source": [
    "### Plot distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467a58df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T08:46:50.444687Z",
     "start_time": "2023-07-23T08:46:50.160497Z"
    }
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "def plot_distribution(data, column_name):\n",
    "    \"\"\"\n",
    "    Plots the distribution of a pandas column using Plotly.\n",
    "\n",
    "    Parameters:\n",
    "        data (pd.DataFrame): The pandas DataFrame containing the data.\n",
    "        column_name (str): The name of the column to plot.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Ensure the column exists in the DataFrame\n",
    "    if column_name not in data.columns:\n",
    "        raise ValueError(f\"Column '{column_name}' not found in the DataFrame.\")\n",
    "\n",
    "    # Use Plotly Express to plot the distribution\n",
    "    fig = px.histogram(data, x=column_name, nbins=50, title=f'Distribution of {column_name}')\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edab1ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T08:47:13.957577Z",
     "start_time": "2023-07-23T08:46:50.595656Z"
    }
   },
   "outputs": [],
   "source": [
    "for col in adjusted_df.columns:\n",
    "    plot_distribution(adjusted_df, col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15746c69",
   "metadata": {},
   "source": [
    "### Plot the two correlation matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8c7108",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T08:47:14.096975Z",
     "start_time": "2023-07-23T08:47:14.084389Z"
    }
   },
   "outputs": [],
   "source": [
    "possible_values.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c86ddc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T08:47:14.808072Z",
     "start_time": "2023-07-23T08:47:14.211015Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ... (Code from the previous answer)\n",
    "\n",
    "# Calculate the correlation matrix for the adjusted DataFrame\n",
    "correlation_matrix_adjusted = adjusted_df.corr()\n",
    "\n",
    "\n",
    "# Display the correlation matrix\n",
    "print(\"Adjusted Correlation Matrix:\")\n",
    "print(correlation_matrix_adjusted)\n",
    "\n",
    "# Convert the original correlation data (corr) to a DataFrame\n",
    "original_corr_df = pd.DataFrame(corr)\n",
    "original_corr_df.set_index('features', inplace=True)\n",
    "\n",
    "# Display the original correlation data\n",
    "print(\"\\nOriginal Correlation Matrix:\")\n",
    "print(original_corr_df)\n",
    "\n",
    "# Plot the correlation matrix heatmaps for both adjusted and original data side by side\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.suptitle(\"Comparison of Correlation Matrices\", fontsize=16)\n",
    "\n",
    "# Adjusted correlation matrix heatmap\n",
    "axes[0].imshow(correlation_matrix_adjusted, cmap='coolwarm', interpolation='nearest')\n",
    "axes[0].set_xticks(np.arange(len(correlation_matrix_adjusted)))\n",
    "axes[0].set_yticks(np.arange(len(correlation_matrix_adjusted)))\n",
    "axes[0].set_xticklabels(correlation_matrix_adjusted.columns, rotation=45)\n",
    "axes[0].set_yticklabels(correlation_matrix_adjusted.columns)\n",
    "axes[0].set_title(\"Adjusted Correlation Matrix\")\n",
    "\n",
    "# Original correlation matrix heatmap\n",
    "axes[1].imshow(original_corr_df, cmap='coolwarm', interpolation='nearest')\n",
    "axes[1].set_xticks(np.arange(len(original_corr_df)))\n",
    "axes[1].set_yticks(np.arange(len(original_corr_df)))\n",
    "axes[1].set_xticklabels(original_corr_df.columns, rotation=45)\n",
    "axes[1].set_yticklabels(original_corr_df.columns)\n",
    "axes[1].set_title(\"Original Correlation Matrix\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19039758",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2908b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T08:47:14.967662Z",
     "start_time": "2023-07-23T08:47:14.957075Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to measure the distance between two correlation matrices using Frobenius norm\n",
    "def correlation_distance(matrix1, matrix2):\n",
    "    return np.linalg.norm(matrix1 - matrix2, ord='fro')\n",
    "\n",
    "\n",
    "current_distance = correlation_distance(matrix1=correlation_matrix_adjusted, matrix2=original_corr_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6126576b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T08:47:15.140669Z",
     "start_time": "2023-07-23T08:47:15.127608Z"
    }
   },
   "outputs": [],
   "source": [
    "current_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c842bd0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "130e5bad",
   "metadata": {},
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d65004",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T08:47:15.298495Z",
     "start_time": "2023-07-23T08:47:15.285497Z"
    }
   },
   "outputs": [],
   "source": [
    "# adjusted_df.to_csv(\"adjusted_df_num_No_BR_250k_v3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b60b03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "15c6a759",
   "metadata": {},
   "source": [
    "## Combine the two datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31f5f29",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T08:48:48.549675Z",
     "start_time": "2023-07-23T08:48:45.508096Z"
    }
   },
   "outputs": [],
   "source": [
    "num_df = pd.read_csv(\"adjusted_df_num_No_BR_250k.csv\", index_col=[0])\n",
    "cat_df = pd.read_csv(\"df_cat_No_Br_250k.csv\", index_col=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c0771e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T08:55:17.390515Z",
     "start_time": "2023-07-23T08:55:17.281253Z"
    }
   },
   "outputs": [],
   "source": [
    "df_merged = pd.concat([num_df, cat_df], axis=1)\n",
    "df_merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f0edd6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T08:55:19.180352Z",
     "start_time": "2023-07-23T08:55:19.121834Z"
    }
   },
   "outputs": [],
   "source": [
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e37b43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "338fe1cd",
   "metadata": {},
   "source": [
    "## Apply the business rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed177786",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T15:09:17.804259Z",
     "start_time": "2023-07-22T15:09:17.789259Z"
    }
   },
   "outputs": [],
   "source": [
    "df_merged.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4e39eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T15:09:17.962260Z",
     "start_time": "2023-07-22T15:09:17.948260Z"
    }
   },
   "outputs": [],
   "source": [
    "# 100 реда, 80 със статус Married и house_memb > 2, 20 статус Married и house_memb <=2;\n",
    "# задачата е да променим статуса на тия 20 реда.\n",
    "\n",
    "# т.е. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211f4e15",
   "metadata": {},
   "source": [
    "100 rows, 20 non_comply, replace 20 with complying values, concatenate back to the 100, out of those 100 select other 20 that don't correspond to the independent feature value of the B_RULE and replace the corresponding values with  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ae2ea8",
   "metadata": {},
   "source": [
    "Plan to apply business rules:\n",
    "\n",
    "- list all columns that are going to be affected\n",
    "- separate the rest (will preserve their distributions and num of examples\n",
    "- sort the rules in a way that the dependent features are repeated (if applicable). In this way we are only generating new data only after all operations on this column are done. \n",
    "- business rules should be applied as follows:\n",
    "\n",
    "    - Select the first and independent and dependent features\n",
    "        - apply the business rule\n",
    "            - save the rows that comply with the business rule - comply_df\n",
    "            - save the rows that DO NOT comply - non_comply_df\n",
    "                - out of them concatenate the independent feature to the comply_df (this will leave n number of NaN's on the dependent feature side)\n",
    "                - count the number of NaN's left (same as num of rows in the non_comply_df\n",
    "                - use it as a value for n_samples\n",
    "                - generate that many n_samples for the same dependent feature (problem, how to assure that it follows the same old distribution??)\n",
    "\n",
    "\n",
    "- Option 2:\n",
    "    - instead of replacing the values with NaN's, replace them with other values that comply with the business rules\n",
    "        - HOW: save the num of rows that DO NOT comply with the business rules\n",
    "        - generate a unique list of all other values that comply\n",
    "        - replace the NaN's (or the current value in the non_comply_df) with random distribution from the other values\n",
    "        - go to the comply_df and also replace the same number of value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdadf326",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T15:09:19.070259Z",
     "start_time": "2023-07-22T15:09:18.092260Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Business rules as a list of dictionaries\n",
    "full_business_rules = [\n",
    "    {\"Independent feature\": \"marit_stat\", \"Independent feature value\": \"=='Married'\", \"Dependent feature\": \"house_memb\", \"Dependent feature value filter\": \">'2'\", \"Note\": \"The number of household members in family households is more likely to be greater than 2\"},\n",
    "    {\"Independent feature\": \"prof_ind\", \"Independent feature value\": \"=='Financial and administrative activities'\", \"Dependent feature\": \"invest_exp\", \"Dependent feature value filter\": \">'0'\", \"Note\": \"They are more likely to own a bank account\"},\n",
    "    {\"Independent feature\": \"age\", \"Independent feature value\": \"<25\", \"Dependent feature\": \"invest_exp\", \"Dependent feature value filter\": \"=='0'\", \"Note\": \"Under 24s are less likely to have investment experience. Between 35-44 and 45-54 are more likely to have extensive investment experience\"},\n",
    "    {\"Independent feature\": \"age\", \"Independent feature value\": \"<25\", \"Dependent feature\": \"lv_educ\", \"Dependent feature value filter\": \"!='Higher'\", \"Note\": \"Under 24s are less likely to have a college degree\"},\n",
    "    {\"Independent feature\": \"age\", \"Independent feature value\": \"<25\", \"Dependent feature\": \"chil_u_18_y\", \"Dependent feature value filter\": \"<'2'\", \"Note\": \"From 20-24, it is less likely to have more than 1 child under 18\"},\n",
    "    {\"Independent feature\": \"invest_exp\", \"Independent feature value\": \">'0'\", \"Dependent feature\": \"deposits\", \"Dependent feature value filter\": \"=='Y'\", \"Note\": \"They are more likely to own a bank account\"},\n",
    "    {\"Independent feature\": \"shares\", \"Independent feature value\": \"=='Y'\", \"Dependent feature\": \"invest_exp\", \"Dependent feature value filter\": \">'0'\", \"Note\": \"Previous investment experience in years\"},\n",
    "    {\"Independent feature\": \"corp_oblig\", \"Independent feature value\": \"=='Y'\", \"Dependent feature\": \"invest_exp\", \"Dependent feature value filter\": \">'0'\", \"Note\": \"Previous investment experience in years\"},\n",
    "    {\"Independent feature\": \"oth\", \"Independent feature value\": \"=='Y'\", \"Dependent feature\": \"invest_exp\", \"Dependent feature value filter\": \">'0'\", \"Note\": \"Previous investment experience in years\"},\n",
    "    {\"Independent feature\": \"inv_fund\", \"Independent feature value\": \"=='Y'\", \"Dependent feature\": \"invest_exp\", \"Dependent feature value filter\": \">'0'\", \"Note\": \"Previous investment experience in years\"},\n",
    "    {\"Independent feature\": \"cash\", \"Independent feature value\": \"=='Y'\", \"Dependent feature\": \"invest_exp\", \"Dependent feature value filter\": \">'0'\", \"Note\": \"Previous investment experience in years\"},\n",
    "    {\"Independent feature\": \"crypto\", \"Independent feature value\": \"=='Y'\", \"Dependent feature\": \"invest_exp\", \"Dependent feature value filter\": \">'0'\", \"Note\": \"Previous investment experience in years\"},\n",
    "    {\"Independent feature\": \"gov_bond\", \"Independent feature value\": \"=='Y'\", \"Dependent feature\": \"invest_exp\", \"Dependent feature value filter\": \">'0'\", \"Note\": \"Previous investment experience in years\"},\n",
    "    {\"Independent feature\": \"age\", \"Independent feature value\": \"<25\", \"Dependent feature\": \"bk_acc\", \"Dependent feature value filter\": \"=='N'\", \"Note\": \"Under 24s are less likely to have a checking account\"},\n",
    "    {\"Independent feature\": \"age\", \"Independent feature value\": \"<18\", \"Dependent feature\": \"bk_acc\", \"Dependent feature value filter\": \"=='N'\", \"Note\": \"Under 18 is not possible to have a current account\"},\n",
    "    {\"Independent feature\": \"lv_educ\", \"Independent feature value\": \"=='Higher'\", \"Dependent feature\": \"income\", \"Dependent feature value filter\": \">27601\", \"Note\": \"A higher level of education implies earnings in the upper range\"},\n",
    "    {\"Independent feature\": \"chil_u_18_y\", \"Independent feature value\": \">'1'\", \"Dependent feature\": \"house_memb\", \"Dependent feature value filter\": \">'3'\", \"Note\": \"The number of household members is directly dependent on the number of children under 18\"},\n",
    "    {\"Independent feature\": \"lv_educ\", \"Independent feature value\": \"=='Higher'\", \"Dependent feature\": \"soc_econ_stat\", \"Dependent feature value filter\": \"=='Economically active'\", \"Note\": \"A higher level of education implies an economically active status\"},\n",
    "    {\"Independent feature\": \"income\", \"Independent feature value\": \">27601\", \"Dependent feature\": \"taxes\", \"Dependent feature value filter\": \">2500\", \"Note\": \"Earnings in the upper range correspond to higher taxes and insurance\"},\n",
    "]\n",
    "\n",
    "\n",
    "# Function to apply a single business rule\n",
    "def apply_business_rule(rule, dataframe):\n",
    "    independent_feature = rule[\"Independent feature\"]\n",
    "    independent_feature_value = rule[\"Independent feature value\"]\n",
    "    dependent_feature = rule[\"Dependent feature\"]\n",
    "    dependent_feature_value_filter = rule[\"Dependent feature value filter\"]\n",
    "\n",
    "    # Construct the filter condition dynamically using f-strings\n",
    "#     filter_condition = f\"(dataframe['marit_stat'] == 'Married') & (dataframe['house_memb'] > {dependent_feature_value_filter})\"\n",
    "#     filter_condition = f\"(dataframe['age'] {independent_feature_value}) & (dataframe['invest_exp']  {dependent_feature_value_filter})\"\n",
    "#         ({independent_feature} {independent_feature_value}) & ({dependent_feature} {dependent_feature_value_filter}))\"\n",
    "\n",
    "    filter_condition = f\"[(dataframe['{independent_feature}'] {independent_feature_value}) & (dataframe['{dependent_feature}'] {dependent_feature_value_filter})]\"   \n",
    "#     [(df_merged['marit_stat'] =='Married') & (df_merged['house_memb'] >'2')]\n",
    "    \n",
    "    print(\"filter_condition: \", filter_condition)\n",
    "    \n",
    "    list_mask = eval(filter_condition)\n",
    "\n",
    "    \n",
    "    \n",
    "    # Apply the filter condition to the DataFrame\n",
    "#     filtered_df = df_merged.loc[eval(filter_condition)]\n",
    "    filtered_df = df_merged[list_mask[0]]\n",
    "\n",
    "    return filtered_df\n",
    "\n",
    "# Apply all business rules to the DataFrame\n",
    "filtered_dfs = []\n",
    "for rule in full_business_rules:\n",
    "#     print(\"Rule: \", rule)\n",
    "    df_filtered = apply_business_rule(rule, df_merged)\n",
    "    \n",
    "    # CHECK DISTRIBUTION OF THE NEW INDEPENDENT VARIABLE\n",
    "    # CHECK DISTRIBUTION OF THE OLD INDEPENDENT VARIABLE\n",
    "    \n",
    "    # IF DIFFERENT\n",
    "        # ADJUST THE NEW ONE TO FOLLOW THE OLD ONE\n",
    "        \n",
    "    # DO THE SAME WITH THE DEPENDENT VARIABLE\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    filtered_dfs.append(df_filtered)\n",
    "\n",
    "# Concatenate all filtered DataFrames\n",
    "all_filtered_dfs = pd.concat(filtered_dfs)\n",
    "df_BR_applied = all_filtered_dfs.drop_duplicates()\n",
    "\n",
    "# print(final_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab71b8f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T15:09:19.450428Z",
     "start_time": "2023-07-22T15:09:19.420262Z"
    }
   },
   "outputs": [],
   "source": [
    "df_BR_applied"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca38e98",
   "metadata": {},
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0e3112",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T15:09:19.815450Z",
     "start_time": "2023-07-22T15:09:19.801428Z"
    }
   },
   "outputs": [],
   "source": [
    "# df_BR_applied.to_csv(\"df_BR_applied_v2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2443a83f",
   "metadata": {},
   "source": [
    "## Check distributions (cat features) and corr matrix (num variables) after Business rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aabc430",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T15:09:21.367503Z",
     "start_time": "2023-07-22T15:09:20.706576Z"
    }
   },
   "outputs": [],
   "source": [
    "df_BR_applied = pd.read_csv(\"df_BR_applied_v2.csv\", index_col=\"Unnamed: 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badba722",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T15:09:21.731563Z",
     "start_time": "2023-07-22T15:09:21.716505Z"
    }
   },
   "outputs": [],
   "source": [
    "num_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fc5238",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T15:09:22.110753Z",
     "start_time": "2023-07-22T15:09:22.080567Z"
    }
   },
   "outputs": [],
   "source": [
    "num_cols = ['age', 'ind_risk', 'income', 'pers_exp', 'house_exp', 'taxes',\n",
    "       'transp_telecom', 'hobby']\n",
    "\n",
    "cat_cols = [col for col in df_merged.columns if col not in num_cols]\n",
    "\n",
    "df_BR_num = df_BR_applied.loc[:, num_cols]\n",
    "df_BR_cat = df_BR_applied.loc[:, cat_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d450173c",
   "metadata": {},
   "source": [
    "### Categotical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c36e6db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T15:09:22.659661Z",
     "start_time": "2023-07-22T15:09:22.645660Z"
    }
   },
   "outputs": [],
   "source": [
    "df_BR_cat[\"house_memb\"].value_counts(normalize=True).sort_index()\n",
    "# 0.1805,0.3778,0.2387,0.1157,0.0525,0.0238,0.011"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4323a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T15:09:24.113334Z",
     "start_time": "2023-07-22T15:09:23.129659Z"
    }
   },
   "outputs": [],
   "source": [
    "categorical_ks_statistics = kolmogorov_smirnov_test(categorical_data=df_BR_cat,\n",
    "                                                   old_distributions=dists)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43268e0",
   "metadata": {},
   "source": [
    "### Numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0471cf0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T15:09:29.664581Z",
     "start_time": "2023-07-22T15:09:24.478355Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "for col in df_BR_num.columns:\n",
    "    plot_distribution(df_BR_num, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbac88c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T15:09:30.295610Z",
     "start_time": "2023-07-22T15:09:30.042581Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ... (Code from the previous answer)\n",
    "\n",
    "# Calculate the correlation matrix for the adjusted DataFrame\n",
    "correlation_matrix_adjusted = df_BR_num.corr()\n",
    "\n",
    "\n",
    "# Display the correlation matrix\n",
    "print(\"Adjusted Correlation Matrix:\")\n",
    "print(correlation_matrix_adjusted)\n",
    "\n",
    "# Convert the original correlation data (corr) to a DataFrame\n",
    "original_corr_df = pd.DataFrame(corr)\n",
    "original_corr_df.set_index('features', inplace=True)\n",
    "\n",
    "# Display the original correlation data\n",
    "print(\"\\nOriginal Correlation Matrix:\")\n",
    "print(original_corr_df)\n",
    "\n",
    "# Plot the correlation matrix heatmaps for both adjusted and original data side by side\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.suptitle(\"Comparison of Correlation Matrices\", fontsize=16)\n",
    "\n",
    "# Adjusted correlation matrix heatmap\n",
    "axes[0].imshow(correlation_matrix_adjusted, cmap='coolwarm', interpolation='nearest')\n",
    "axes[0].set_xticks(np.arange(len(correlation_matrix_adjusted)))\n",
    "axes[0].set_yticks(np.arange(len(correlation_matrix_adjusted)))\n",
    "axes[0].set_xticklabels(correlation_matrix_adjusted.columns, rotation=45)\n",
    "axes[0].set_yticklabels(correlation_matrix_adjusted.columns)\n",
    "axes[0].set_title(\"Adjusted Correlation Matrix\")\n",
    "\n",
    "# Original correlation matrix heatmap\n",
    "axes[1].imshow(original_corr_df, cmap='coolwarm', interpolation='nearest')\n",
    "axes[1].set_xticks(np.arange(len(original_corr_df)))\n",
    "axes[1].set_yticks(np.arange(len(original_corr_df)))\n",
    "axes[1].set_xticklabels(original_corr_df.columns, rotation=45)\n",
    "axes[1].set_yticklabels(original_corr_df.columns)\n",
    "axes[1].set_title(\"Original Correlation Matrix\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1344748",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a48515",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a93816",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dfdd6287",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3c6bec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T09:02:26.426569Z",
     "start_time": "2023-07-23T09:02:25.644424Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"df_BR_applied_v2.csv\", index_col=\"Unnamed: 0\")\n",
    "# data_sample = df.sample(n=100)\n",
    "data_sample = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92070d58",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T09:02:26.852357Z",
     "start_time": "2023-07-23T09:02:26.830820Z"
    }
   },
   "outputs": [],
   "source": [
    "df[\"mortgage\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51c2db6",
   "metadata": {},
   "source": [
    "## Data Binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e26d235",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T09:10:49.285923Z",
     "start_time": "2023-07-23T09:10:49.279385Z"
    }
   },
   "outputs": [],
   "source": [
    "cat_cols = list(dists.keys())\n",
    "print(\"few categorical columns: \", cat_cols[:5])\n",
    "num_cols = [col for col in df.columns if col not in cat_cols]\n",
    "print(\"few numerical columns: \", num_cols[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505cc93a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a6247a5",
   "metadata": {},
   "source": [
    "### Bin the numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b39326",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T09:06:22.023345Z",
     "start_time": "2023-07-23T09:06:22.008318Z"
    }
   },
   "outputs": [],
   "source": [
    "data_sample_num = data_sample[num_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee5d1f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T09:06:25.600402Z",
     "start_time": "2023-07-23T09:06:25.574206Z"
    }
   },
   "outputs": [],
   "source": [
    "data_sample_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfaed8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T09:06:44.144369Z",
     "start_time": "2023-07-23T09:06:44.100261Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Function to bin a feature based on the provided bins\n",
    "def bin_feature(df, feature, bins):\n",
    "    df[feature + '_binned'] = pd.cut(df[feature], bins=bins, labels=False)\n",
    "    return df\n",
    "\n",
    "# Binning specifications from the provided table\n",
    "bins_info = {\n",
    "    'age': [20, 25, 35, 50, 65, 85, float('inf')],\n",
    "    'ind_risk': [0, 0.2, 0.4, 0.6, 0.8, 1, float('inf')],\n",
    "    'income': [0, 6121, 12001, 27601, 43201, 58801, 74401, float('inf')],\n",
    "    'pers_exp': [0, 4500, 5000, 5500, float('inf')],\n",
    "    'house_exp': [0, 500, 1500, 3000, float('inf')],\n",
    "    'taxes': [0, 500, 1000, 2000, 2500, float('inf')],\n",
    "    'transp_telecom': [0, 500, 1000, 1500, 2500, float('inf')],\n",
    "    'hobby': [0, 1500, 2000, 3000, float('inf')],\n",
    "}\n",
    "\n",
    "# Perform binning for each feature\n",
    "for feature, bins in bins_info.items():\n",
    "    num_df_binned = bin_feature(data_sample_num, feature, bins)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a232a1d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T09:07:31.941066Z",
     "start_time": "2023-07-23T09:07:31.925514Z"
    }
   },
   "outputs": [],
   "source": [
    "data_sample_num.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7642dc15",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T09:08:07.227809Z",
     "start_time": "2023-07-23T09:08:07.214811Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "bin_cols = ['age_binned', 'ind_risk_binned',\n",
    "       'income_binned', 'pers_exp_binned', 'house_exp_binned', 'taxes_binned',\n",
    "       'transp_telecom_binned', 'hobby_binned']\n",
    "\n",
    "data_sample_num_bin_only = data_sample_num[bin_cols]\n",
    "data_sample_num_bin_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2fa063",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T09:08:44.278389Z",
     "start_time": "2023-07-23T09:08:44.259362Z"
    }
   },
   "outputs": [],
   "source": [
    "data_sample_num_bin_only[\"age_binned\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc759ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T09:08:46.239891Z",
     "start_time": "2023-07-23T09:08:46.228893Z"
    }
   },
   "outputs": [],
   "source": [
    "data_sample_num_bin_only[\"hobby_binned\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acce494a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T09:09:54.893306Z",
     "start_time": "2023-07-23T09:09:54.854780Z"
    }
   },
   "outputs": [],
   "source": [
    "data_sample.drop(num_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9638ccf4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T09:10:21.265211Z",
     "start_time": "2023-07-23T09:10:21.246671Z"
    }
   },
   "outputs": [],
   "source": [
    "data_sample = pd.concat([data_sample, data_sample_num_bin_only], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9546d13",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T09:10:49.285923Z",
     "start_time": "2023-07-23T09:10:49.279385Z"
    }
   },
   "outputs": [],
   "source": [
    "cat_cols = list(dists.keys())\n",
    "print(\"few categorical columns: \", cat_cols[:5])\n",
    "num_cols = [col for col in data_sample.columns if col not in cat_cols]\n",
    "print(\"few numerical columns: \", num_cols[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bc6b71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3ab2edb",
   "metadata": {},
   "source": [
    "## Data encoding (categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4dac65e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T09:10:53.172626Z",
     "start_time": "2023-07-23T09:10:53.154599Z"
    }
   },
   "outputs": [],
   "source": [
    "def data_encoding(data, categorical_cols):\n",
    "    \n",
    "    return pd.get_dummies(data, columns=categorical_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54343327",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T09:11:05.044995Z",
     "start_time": "2023-07-23T09:11:04.644720Z"
    }
   },
   "outputs": [],
   "source": [
    "encoded_data_sample = data_encoding(data_sample, categorical_cols=cat_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422eb156",
   "metadata": {},
   "source": [
    "## Feature scaling (data standardization - numerical)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770da2e5",
   "metadata": {},
   "source": [
    "min-max scaling and standardization (z-score normalization)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614d6f84",
   "metadata": {},
   "source": [
    "### NOOO- Split the data into numerical and categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f1bad0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T09:11:26.446803Z",
     "start_time": "2023-07-23T09:11:26.437296Z"
    }
   },
   "outputs": [],
   "source": [
    "def data_split_cat_num(data, numerical_cols):\n",
    "    \n",
    "    num_data = data[numerical_cols]\n",
    "    cat_cols = [col for col in data.columns if col not in numerical_cols]\n",
    "    cat_data = data[cat_cols]\n",
    "    \n",
    "    cat_data.reset_index(inplace=True, drop=True)\n",
    "    num_data.reset_index(inplace=True, drop=True)\n",
    "    return cat_data, num_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45f4dfd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T09:11:27.051125Z",
     "start_time": "2023-07-23T09:11:27.016097Z"
    }
   },
   "outputs": [],
   "source": [
    "encoded_cat_data, encoded_num_data = data_split_cat_num(data=encoded_data_sample, \n",
    "                                                       numerical_cols=num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239dcee3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T09:11:27.411143Z",
     "start_time": "2023-07-23T09:11:27.407142Z"
    }
   },
   "outputs": [],
   "source": [
    "def data_standardization(data):\n",
    "    \n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    # Initialize the StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Fit and transform the DataFrame to perform standardization\n",
    "    standardized_df = pd.DataFrame(scaler.fit_transform(data), columns=data.columns)\n",
    "    \n",
    "    return standardized_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ef0faf",
   "metadata": {},
   "source": [
    "test standardization with all features (not just the numerical ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e239e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T09:11:28.221060Z",
     "start_time": "2023-07-23T09:11:28.198028Z"
    }
   },
   "outputs": [],
   "source": [
    "encoded_data_sample_standardized_numerical = data_standardization(encoded_num_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bf3c42",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T09:11:28.646577Z",
     "start_time": "2023-07-23T09:11:28.600066Z"
    }
   },
   "outputs": [],
   "source": [
    "encoded_cat_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4940e10",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T09:11:29.073783Z",
     "start_time": "2023-07-23T09:11:29.057785Z"
    }
   },
   "outputs": [],
   "source": [
    "encoded_data_sample_standardized_numerical.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f12593f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T09:11:29.503801Z",
     "start_time": "2023-07-23T09:11:29.495288Z"
    }
   },
   "outputs": [],
   "source": [
    "encoded_data_sample_standardized = pd.concat([encoded_cat_data, encoded_data_sample_standardized_numerical],\n",
    "                                            axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569ba239",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T09:11:29.931779Z",
     "start_time": "2023-07-23T09:11:29.918262Z"
    }
   },
   "outputs": [],
   "source": [
    "encoded_data_sample_standardized.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b957bc0d",
   "metadata": {},
   "source": [
    "## Delete binary cols (columns that have only two possible options as values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570536d0",
   "metadata": {},
   "source": [
    "Will keep only the \"YES\" features (e.g. mortgage_YES - a value of 1 here means the client wants to have a mortgage, 0 - otherwise) to reduce the size of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51e63e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T09:11:31.575624Z",
     "start_time": "2023-07-23T09:11:31.557096Z"
    }
   },
   "outputs": [],
   "source": [
    "def del_NO_cols(data, additional_cols_to_del=['banking_Offline', 'own_rent_house_my own', 'soc_econ_stat_Economically inactive', 'sex_F']):\n",
    "    \n",
    "    # select all \"_NO\" columns \n",
    "    cols_to_del = list(data.filter(regex='_NO$').columns)\n",
    "    \n",
    "    # and extend the list with additional columns to be deleted\n",
    "    cols_to_del.extend(additional_cols_to_del)\n",
    "    \n",
    "    reduced_data = data.drop(cols_to_del, axis=1)\n",
    "    \n",
    "    return reduced_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd7d253",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T09:11:33.381831Z",
     "start_time": "2023-07-23T09:11:33.366289Z"
    }
   },
   "outputs": [],
   "source": [
    "encoded_data_sample_reduced = del_NO_cols(encoded_data_sample_standardized)\n",
    "# encoded_data_sample_reduced = del_NO_cols(encoded_data_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e392de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T09:11:33.843862Z",
     "start_time": "2023-07-23T09:11:33.828831Z"
    }
   },
   "outputs": [],
   "source": [
    "encoded_data_sample_reduced.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d526eef4",
   "metadata": {},
   "source": [
    "## NOO Split data - predictors and target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21f26ab",
   "metadata": {},
   "source": [
    "Target features:\n",
    "\n",
    "    Overdraft\n",
    "    Consumer credit\n",
    "    Mortgage loan\n",
    "    Credit card\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178bbe23",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T09:11:35.636074Z",
     "start_time": "2023-07-23T09:11:35.617074Z"
    }
   },
   "outputs": [],
   "source": [
    "encoded_data_sample_reduced.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05187cee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T09:11:37.427812Z",
     "start_time": "2023-07-23T09:11:37.413784Z"
    }
   },
   "outputs": [],
   "source": [
    "target_columns = [\"overdraft_YES\", \"cons_cred_YES\", \"mortgage_YES\", \"bk_cc_YES\"]\n",
    "\n",
    "def split_pred_target(data, target_cols):\n",
    "\n",
    "    predictor_cols = [col for col in data.columns if col not in target_cols]\n",
    "\n",
    "    \n",
    "    X_data = data[predictor_cols]\n",
    "    y_data = data[target_cols]\n",
    "\n",
    "    return X_data, y_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04065ffe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T09:11:37.882805Z",
     "start_time": "2023-07-23T09:11:37.867806Z"
    }
   },
   "outputs": [],
   "source": [
    "X, y = split_pred_target(data=encoded_data_sample_reduced, target_cols=target_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4288347",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e7c48aed",
   "metadata": {},
   "source": [
    "## Split data on train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf7da50",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T09:11:40.081176Z",
     "start_time": "2023-07-23T09:11:39.374177Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6877be",
   "metadata": {},
   "source": [
    "##### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c029319a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T09:11:43.257829Z",
     "start_time": "2023-07-23T09:11:40.591177Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train.to_csv(\"X_train_v1.csv\")\n",
    "X_test.to_csv(\"X_test_v1.csv\")\n",
    "y_train.to_csv(\"y_train_v1.csv\")\n",
    "y_test.to_csv(\"y_test_v1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368be442",
   "metadata": {},
   "source": [
    "### Test distributions of train vs test data (Categorical) - Kolmogorov-Smirnov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538fd9f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T09:11:43.782938Z",
     "start_time": "2023-07-23T09:11:43.768832Z"
    }
   },
   "outputs": [],
   "source": [
    "num_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f441a3d",
   "metadata": {},
   "source": [
    "#### Generate encoded categorical distributions for the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a7f417",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T09:11:44.463247Z",
     "start_time": "2023-07-23T09:11:44.307234Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_dists = {}\n",
    "\n",
    "cat_cols_encoded = [col for col in X.columns if col not in num_cols]\n",
    "\n",
    "for col in cat_cols_encoded:\n",
    "    \n",
    "    X_train_dists[col] = {'labels': list(X_train[col].value_counts().index), \n",
    "                          'values':list(X_train[col].value_counts(normalize=True).values)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e518bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T09:11:44.985836Z",
     "start_time": "2023-07-23T09:11:44.970765Z"
    }
   },
   "outputs": [],
   "source": [
    "# X_train_dists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f64434",
   "metadata": {},
   "source": [
    "#### Perform the test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce00591",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T09:11:45.780840Z",
     "start_time": "2023-07-23T09:11:45.494839Z"
    }
   },
   "outputs": [],
   "source": [
    "kolmogorov_smirnov_test(X_test[cat_cols_encoded], X_train_dists)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d48bbba",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0faaba8",
   "metadata": {},
   "source": [
    "### Numerical data checks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b996b7",
   "metadata": {},
   "source": [
    "#### Check distributions of train vs test numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600a53f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T09:11:58.387017Z",
     "start_time": "2023-07-23T09:11:53.722465Z"
    }
   },
   "outputs": [],
   "source": [
    "for col in X_train[num_cols].columns:\n",
    "    plot_distribution(X_train[num_cols], col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3f8394",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T09:12:01.108006Z",
     "start_time": "2023-07-23T09:11:58.898018Z"
    }
   },
   "outputs": [],
   "source": [
    "for col in X_test[num_cols].columns:\n",
    "    plot_distribution(X_test[num_cols], col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1479f139",
   "metadata": {},
   "source": [
    "####  Compare the corr matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27e7cec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T09:12:06.615459Z",
     "start_time": "2023-07-23T09:12:06.366952Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ... (Code from the previous answer)\n",
    "\n",
    "# Calculate the correlation matrix for the adjusted DataFrame\n",
    "correlation_matrix_adjusted = X_test[num_cols].corr()\n",
    "\n",
    "\n",
    "# Display the correlation matrix\n",
    "print(\"Adjusted Correlation Matrix:\")\n",
    "print(correlation_matrix_adjusted)\n",
    "\n",
    "# Convert the original correlation data (corr) to a DataFrame\n",
    "original_corr_df = pd.DataFrame(X_train[num_cols].corr())\n",
    "# original_corr_df.set_index('features', inplace=True)\n",
    "\n",
    "# Display the original correlation data\n",
    "print(\"\\nOriginal Correlation Matrix:\")\n",
    "print(original_corr_df)\n",
    "\n",
    "# Plot the correlation matrix heatmaps for both adjusted and original data side by side\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.suptitle(\"Comparison of Correlation Matrices\", fontsize=16)\n",
    "\n",
    "# Adjusted correlation matrix heatmap\n",
    "axes[0].imshow(correlation_matrix_adjusted, cmap='coolwarm', interpolation='nearest')\n",
    "axes[0].set_xticks(np.arange(len(correlation_matrix_adjusted)))\n",
    "axes[0].set_yticks(np.arange(len(correlation_matrix_adjusted)))\n",
    "axes[0].set_xticklabels(correlation_matrix_adjusted.columns, rotation=45)\n",
    "axes[0].set_yticklabels(correlation_matrix_adjusted.columns)\n",
    "axes[0].set_title(\"Adjusted Correlation Matrix\")\n",
    "\n",
    "# Original correlation matrix heatmap\n",
    "axes[1].imshow(original_corr_df, cmap='coolwarm', interpolation='nearest')\n",
    "axes[1].set_xticks(np.arange(len(original_corr_df)))\n",
    "axes[1].set_yticks(np.arange(len(original_corr_df)))\n",
    "axes[1].set_xticklabels(original_corr_df.columns, rotation=45)\n",
    "axes[1].set_yticklabels(original_corr_df.columns)\n",
    "axes[1].set_title(\"Original Correlation Matrix\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba28fed6",
   "metadata": {},
   "source": [
    "## Generate new features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d70b16",
   "metadata": {},
   "source": [
    "### Generate Non-Linear features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17822582",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T09:12:14.312763Z",
     "start_time": "2023-07-23T09:12:14.301736Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3117ed00",
   "metadata": {},
   "source": [
    "#### Logarithmic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913b8ea4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T09:12:15.014388Z",
     "start_time": "2023-07-23T09:12:14.999365Z"
    }
   },
   "outputs": [],
   "source": [
    "orig_cols = X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1d2a37",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T09:12:16.672243Z",
     "start_time": "2023-07-23T09:12:15.590390Z"
    }
   },
   "outputs": [],
   "source": [
    "for col in orig_cols:\n",
    "    \n",
    "    new_col_name = col + \"_log\"\n",
    "\n",
    "    X_train[new_col_name] = np.log(X_train[col])\n",
    "    X_test[new_col_name] = np.log(X_test[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae093504",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T09:12:17.686271Z",
     "start_time": "2023-07-23T09:12:17.607245Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train.info(verbose=True, show_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989117fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "847c9b69",
   "metadata": {},
   "source": [
    "#### Quadratic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ea111b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T09:12:18.508784Z",
     "start_time": "2023-07-23T09:12:18.385272Z"
    }
   },
   "outputs": [],
   "source": [
    "for col in orig_cols:\n",
    "    \n",
    "    new_col_name = col + \"_x2\"\n",
    "\n",
    "    X_train[new_col_name] = X_train[col]**2\n",
    "    X_test[new_col_name] = X_test[col]**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa59fac6",
   "metadata": {},
   "source": [
    "#### Reciprocal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f87ac1d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T09:12:19.477911Z",
     "start_time": "2023-07-23T09:12:19.303784Z"
    }
   },
   "outputs": [],
   "source": [
    "for col in orig_cols:\n",
    "    \n",
    "    new_col_name = col + \"_1/x\"\n",
    "\n",
    "    X_train[new_col_name] = 1/X_train[col]\n",
    "    X_test[new_col_name] = 1/X_test[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb87bccb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9ddf6235",
   "metadata": {},
   "source": [
    "#### Exponential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8410e7b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T09:12:20.876985Z",
     "start_time": "2023-07-23T09:12:20.464913Z"
    }
   },
   "outputs": [],
   "source": [
    "for col in orig_cols:\n",
    "    \n",
    "    new_col_name = col + \"_exp\"\n",
    "\n",
    "    X_train[new_col_name] = np.exp(X_train[col])\n",
    "    X_test[new_col_name] = np.exp(X_test[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3f622f",
   "metadata": {},
   "source": [
    "#### Square rooted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8674c3bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T09:12:22.466018Z",
     "start_time": "2023-07-23T09:12:22.055986Z"
    }
   },
   "outputs": [],
   "source": [
    "for col in orig_cols:\n",
    "    \n",
    "    new_col_name = col + \"_sqrt\"\n",
    "\n",
    "    X_train[new_col_name] = np.exp(X_train[col])\n",
    "    X_test[new_col_name] = np.exp(X_test[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe5614d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T09:12:24.100549Z",
     "start_time": "2023-07-23T09:12:23.846020Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train.info(verbose=True, show_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ed603a",
   "metadata": {},
   "source": [
    "#### Drop cols with null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b83f82d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T09:12:25.810206Z",
     "start_time": "2023-07-23T09:12:25.462549Z"
    }
   },
   "outputs": [],
   "source": [
    "columns_with_null = X_train.columns[X_train.isnull().any()]\n",
    "\n",
    "# Then, drop the columns with null values\n",
    "X_train.drop(columns=columns_with_null, axis=1, inplace=True)\n",
    "X_test.drop(columns=columns_with_null, axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66901ce6",
   "metadata": {},
   "source": [
    "### Generate custom features (based on domain knowledge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2196ce93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5dbaa2d5",
   "metadata": {},
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fdc2f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T09:15:02.671699Z",
     "start_time": "2023-07-23T09:15:02.652150Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "y_train_overdraft = y_train[\"overdraft_YES\"]\n",
    "y_train_cons_cred = y_train[\"cons_cred_YES\"]\n",
    "y_train_mortgage  = y_train[\"mortgage_YES\"]\n",
    "y_train_bk_cc     = y_train[\"bk_cc_YES\"]\n",
    "\n",
    "\n",
    "\n",
    "y_test_overdraft = y_test[\"overdraft_YES\"]\n",
    "y_test_cons_cred = y_test[\"cons_cred_YES\"]\n",
    "y_test_mortgage  = y_test[\"mortgage_YES\"]\n",
    "y_test_bk_cc     = y_test[\"bk_cc_YES\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e538be05",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T09:12:27.318211Z",
     "start_time": "2023-07-23T09:12:27.305236Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48206a55",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T09:12:28.734175Z",
     "start_time": "2023-07-23T09:12:28.575268Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train.info(verbose=True, show_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223cf86d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T07:39:41.500941Z",
     "start_time": "2023-07-23T07:39:41.487914Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057cf7fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T07:39:41.548331Z",
     "start_time": "2023-07-23T07:39:41.504476Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac23d4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c87a14",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T09:12:30.635536Z",
     "start_time": "2023-07-23T09:12:29.998180Z"
    }
   },
   "outputs": [],
   "source": [
    "def drop_columns_with_infinity(df):\n",
    "    # Step 1: Identify columns containing infinity\n",
    "    cols_with_infinity = df.columns[df.isin([np.inf, -np.inf]).any()]\n",
    "\n",
    "    # Step 2: Drop the identified columns\n",
    "    df.drop(columns=cols_with_infinity, inplace=True)\n",
    "\n",
    "# Example usage:\n",
    "# Assuming your DataFrame is named 'df'\n",
    "drop_columns_with_infinity(X_train)\n",
    "drop_columns_with_infinity(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91019f71",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T09:12:31.610118Z",
     "start_time": "2023-07-23T09:12:31.595597Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2447a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T09:12:32.559614Z",
     "start_time": "2023-07-23T09:12:32.545121Z"
    }
   },
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bb05f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T09:12:33.497645Z",
     "start_time": "2023-07-23T09:12:33.483614Z"
    }
   },
   "outputs": [],
   "source": [
    "num_features = num_cols\n",
    "cat_features = [col for col in X_train.columns if col not in num_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e9e4ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "58bece68",
   "metadata": {},
   "source": [
    "### Select numerical features (ANOVA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e4efa1",
   "metadata": {},
   "source": [
    "##### Backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1aaad5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T09:12:34.434568Z",
     "start_time": "2023-07-23T09:12:34.419648Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_backup = X_train\n",
    "X_test_backup = X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc9fc21",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T09:12:35.870582Z",
     "start_time": "2023-07-23T09:12:35.855566Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif, chi2\n",
    "\n",
    "pd.options.display.float_format = '{:,.5f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2253d5bd",
   "metadata": {},
   "source": [
    "From the chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c09e05b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T09:12:36.903855Z",
     "start_time": "2023-07-23T09:12:36.892581Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "def anova_feature_selection(X, y, significance_level=0.05):\n",
    "    \"\"\"\n",
    "    Perform feature selection using ANOVA and visualize p-values and feature selection status.\n",
    "\n",
    "    Parameters:\n",
    "        X (pandas DataFrame): The input feature matrix.\n",
    "        y (pandas Series or array-like): The target variable.\n",
    "        significance_level (float): The significance level (default=0.05).\n",
    "\n",
    "    Returns:\n",
    "        pandas DataFrame: A DataFrame containing p-values and feature selection status.\n",
    "    \"\"\"\n",
    "    # Perform ANOVA\n",
    "    f_values, p_values = f_classif(X, y)\n",
    "    is_selected = p_values < significance_level\n",
    "\n",
    "    # Create a DataFrame to store the results\n",
    "    results_df = pd.DataFrame({'Feature': X.columns, 'p-value': p_values, 'Selected': is_selected})\n",
    "\n",
    "    # Sort the DataFrame based on p-values\n",
    "    results_df.sort_values(by='p-value', ascending=True, inplace=True)\n",
    "\n",
    "    # Visualize the p-values and feature selection status\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(results_df['Feature'], -np.log10(results_df['p-value']), color=results_df['Selected'].map({True: 'g', False: 'r'}))\n",
    "    plt.axhline(-np.log10(significance_level), color='b', linestyle='--', label=f'Significance Level ({significance_level})')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.ylabel('-log(p-value)')\n",
    "    plt.title('ANOVA Feature Selection')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return results_df\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "def anova_feature_selection_with_plotly(X, y, significance_level=0.05):\n",
    "    \"\"\"\n",
    "    Perform feature selection using ANOVA and visualize p-values and feature selection status using Plotly.\n",
    "\n",
    "    Parameters:\n",
    "        X (pandas DataFrame): The input feature matrix.\n",
    "        y (pandas Series or array-like): The target variable.\n",
    "        significance_level (float): The significance level (default=0.05).\n",
    "\n",
    "    Returns:\n",
    "        pandas DataFrame: A DataFrame containing p-values and feature selection status.\n",
    "    \"\"\"\n",
    "    # Perform ANOVA\n",
    "    f_values, p_values = f_classif(X, y)\n",
    "    is_selected = p_values < significance_level\n",
    "\n",
    "    # Create a DataFrame to store the results\n",
    "    results_df = pd.DataFrame({'Feature': X.columns, 'p-value': p_values, 'Selected': is_selected})\n",
    "\n",
    "    # Sort the DataFrame based on p-values\n",
    "    results_df.sort_values(by='p-value', ascending=True, inplace=True)\n",
    "\n",
    "    # Visualize the p-values and feature selection status using Plotly\n",
    "    fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "    fig.add_trace(go.Bar(x=results_df['Feature'], y=-np.log10(results_df['p-value']),\n",
    "                         marker_color=results_df['Selected'].map({True: 'green', False: 'red'}),\n",
    "                         name='-log(p-value)'))\n",
    "\n",
    "    fig.add_trace(go.Scatter(x=results_df['Feature'], y=[-np.log10(significance_level)] * len(results_df),\n",
    "                             mode='lines', line=dict(dash='dash', color='blue'),\n",
    "                             name=f'Significance Level ({significance_level})'), secondary_y=True)\n",
    "\n",
    "    fig.update_xaxes(tickangle=90)\n",
    "    fig.update_yaxes(title='-log(p-value)', secondary_y=False)\n",
    "    fig.update_yaxes(title='-log(p-value)', secondary_y=True, showgrid=False)\n",
    "    fig.update_layout(title='ANOVA Feature Selection', legend=dict(x=0, y=1))\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b8bfb9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T09:12:37.956353Z",
     "start_time": "2023-07-23T09:12:37.909279Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_num = X_train.loc[:, num_features]\n",
    "X_train_cat = X_train.loc[:, cat_features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd7b091",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T09:15:21.360332Z",
     "start_time": "2023-07-23T09:15:13.906999Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_cat_anova = anova_feature_selection(X=X_train_cat,\n",
    "                       y=y_train_overdraft,\n",
    "                       significance_level=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f050c9d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T09:15:22.571442Z",
     "start_time": "2023-07-23T09:15:22.557442Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "def anova_feature_selection_with_plotly(X, y, significance_level=0.05):\n",
    "    \"\"\"\n",
    "    Perform feature selection using ANOVA and visualize p-values and feature selection status using Plotly.\n",
    "\n",
    "    Parameters:\n",
    "        X (pandas DataFrame): The input feature matrix.\n",
    "        y (pandas Series or array-like): The target variable.\n",
    "        significance_level (float): The significance level (default=0.05).\n",
    "\n",
    "    Returns:\n",
    "        pandas DataFrame: A DataFrame containing p-values and feature selection status.\n",
    "    \"\"\"\n",
    "    # Perform ANOVA\n",
    "    f_values, p_values = f_classif(X, y)\n",
    "    is_selected = p_values < significance_level\n",
    "\n",
    "    # Create a DataFrame to store the results\n",
    "    results_df = pd.DataFrame({'Feature': X.columns, 'p-value': p_values, 'Selected': is_selected})\n",
    "\n",
    "    # Sort the DataFrame based on p-values\n",
    "    results_df.sort_values(by='p-value', ascending=True, inplace=True)\n",
    "\n",
    "    # Shorten the feature names for better visualization\n",
    "    max_label_length = 15  # Adjust the maximum label length as needed\n",
    "    results_df['Shortened_Feature'] = results_df['Feature'].apply(lambda x: x[:max_label_length] + '...' if len(x) > max_label_length else x)\n",
    "\n",
    "    # Visualize the p-values and feature selection status using Plotly\n",
    "    fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "    fig.add_trace(go.Bar(x=results_df['Shortened_Feature'], y=-np.log10(results_df['p-value']),\n",
    "                         marker_color=results_df['Selected'].map({True: 'green', False: 'red'}),\n",
    "                         name='-log(p-value)'))\n",
    "\n",
    "    fig.add_trace(go.Scatter(x=results_df['Shortened_Feature'],\n",
    "                             y=[-np.log10(significance_level)] * len(results_df),\n",
    "                             mode='lines', line=dict(dash='dash', color='blue'),\n",
    "                             name=f'Significance Level ({significance_level})'), secondary_y=True)\n",
    "\n",
    "    fig.update_xaxes(tickangle=45)  # Rotate the x-axis labels by 45 degrees\n",
    "    fig.update_yaxes(title='-log(p-value)', secondary_y=False)\n",
    "    fig.update_yaxes(title='-log(p-value)', secondary_y=True, showgrid=False)\n",
    "    fig.update_layout(title='ANOVA Feature Selection', legend=dict(x=0, y=1), height=600)  # Increase the height\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea6dbdc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T09:15:37.231594Z",
     "start_time": "2023-07-23T09:15:37.221080Z"
    }
   },
   "outputs": [],
   "source": [
    "# X_train_cat_anova_plotly = anova_feature_selection_with_plotly(X=X_train_cat,\n",
    "#                        y=y_train_overdraft,\n",
    "#                        significance_level=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da98fe5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5583f1b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T09:15:25.516121Z",
     "start_time": "2023-07-23T09:15:25.502869Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_cat_anova"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab24a1d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T09:16:00.841572Z",
     "start_time": "2023-07-23T09:16:00.827536Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_cat_sel_cols = X_train_cat_anova.loc[X_train_cat_anova[\"Selected\"] == True][\"Feature\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f4cc98",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T09:16:02.132635Z",
     "start_time": "2023-07-23T09:16:02.118117Z"
    }
   },
   "outputs": [],
   "source": [
    "list(X_train_cat_sel_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9924e376",
   "metadata": {},
   "source": [
    "### Select categorical features (Chi Squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35b7961",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T09:16:20.193353Z",
     "start_time": "2023-07-23T09:16:20.180574Z"
    }
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.feature_selection import chi2\n",
    "\n",
    "# def chi_squared_feature_selection(X, y, significance_level=0.05):\n",
    "#     \"\"\"\n",
    "#     Perform feature selection using chi-squared test and visualize p-values and feature selection status.\n",
    "\n",
    "#     Parameters:\n",
    "#         X (pandas DataFrame): The input feature matrix.\n",
    "#         y (pandas Series or array-like): The target variable.\n",
    "#         significance_level (float): The significance level (default=0.05).\n",
    "\n",
    "#     Returns:\n",
    "#         pandas DataFrame: A DataFrame containing p-values and feature selection status.\n",
    "#     \"\"\"\n",
    "#     # Perform chi-squared test\n",
    "#     chi2_values, p_values = chi2(X, y)\n",
    "#     is_selected = p_values < significance_level\n",
    "\n",
    "#     # Create a DataFrame to store the results\n",
    "#     results_df = pd.DataFrame({'Feature': X.columns, 'p-value': p_values, 'Selected': is_selected})\n",
    "\n",
    "#     # Sort the DataFrame based on p-values\n",
    "#     results_df.sort_values(by='p-value', ascending=True, inplace=True)\n",
    "\n",
    "#     # Visualize the p-values and feature selection status\n",
    "#     plt.figure(figsize=(12, 6))\n",
    "#     plt.bar(results_df['Feature'], -np.log10(results_df['p-value']), color=results_df['Selected'].map({True: 'g', False: 'r'}))\n",
    "#     plt.axhline(-np.log10(significance_level), color='b', linestyle='--', label=f'Significance Level ({significance_level})')\n",
    "#     plt.xticks(rotation=90)\n",
    "#     plt.ylabel('-log(p-value)')\n",
    "#     plt.title('Chi-Squared Feature Selection')\n",
    "#     plt.legend()\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "#     return results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa25b263",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T09:16:17.861657Z",
     "start_time": "2023-07-23T09:16:17.843593Z"
    }
   },
   "outputs": [],
   "source": [
    "# X_train_chi_cols = chi_squared_feature_selection(X=X_train_num,\n",
    "#                        y=y_train_overdraft,\n",
    "#                        significance_level=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed0105b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb73e857",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T09:16:22.873617Z",
     "start_time": "2023-07-23T09:16:22.857033Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "def pearson_correlation_feature_selection(X, y, significance_level=0.05):\n",
    "    \"\"\"\n",
    "    Perform feature selection using Pearson correlation coefficient and visualize correlations.\n",
    "\n",
    "    Parameters:\n",
    "        X (pandas DataFrame): The input feature matrix (numerical features only).\n",
    "        y (pandas Series or array-like): The target variable (numerical).\n",
    "        significance_level (float): The significance level for p-values (default=0.05).\n",
    "\n",
    "    Returns:\n",
    "        pandas DataFrame: A DataFrame containing Pearson correlation coefficients and p-values.\n",
    "    \"\"\"\n",
    "    # Compute Pearson correlation coefficients and p-values\n",
    "    corr_coeffs, p_values = np.abs(np.corrcoef(X, y, rowvar=False)[-1, :-1]), []\n",
    "    for i in range(X.shape[1]):\n",
    "        _, p_value = pearsonr(X.iloc[:, i], y)\n",
    "        p_values.append(p_value)\n",
    "\n",
    "    # Create a DataFrame to store the results\n",
    "    results_df = pd.DataFrame({'Feature': X.columns, 'Correlation Coefficient': corr_coeffs, 'p-value': p_values})\n",
    "\n",
    "    # Sort the DataFrame based on correlation coefficients\n",
    "    results_df.sort_values(by='Correlation Coefficient', ascending=False, inplace=True)\n",
    "\n",
    "    # Visualize the correlations\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(results_df['Feature'], results_df['Correlation Coefficient'])\n",
    "    plt.axhline(significance_level, color='red', linestyle='--', label=f'Significance Level ({significance_level})')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.ylabel('Correlation Coefficient (absolute)')\n",
    "    plt.title('Pearson Correlation Feature Selection')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0fe45f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T09:16:24.381624Z",
     "start_time": "2023-07-23T09:16:24.192593Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_pears_cols = pearson_correlation_feature_selection(X=X_train_num,\n",
    "               y=y_train_overdraft,\n",
    "                                                       significance_level=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727d1b30",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T09:16:30.119400Z",
     "start_time": "2023-07-23T09:16:30.113400Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_pears_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2dc864",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f16f85",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T09:18:04.667499Z",
     "start_time": "2023-07-23T09:18:04.656984Z"
    }
   },
   "outputs": [],
   "source": [
    "all_st_fs = []\n",
    "all_st_fs.extend(list(X_train_cat_sel_cols))\n",
    "all_st_fs.extend(list(num_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5950a7fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T09:18:05.953915Z",
     "start_time": "2023-07-23T09:18:05.939514Z"
    }
   },
   "outputs": [],
   "source": [
    "all_st_fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab752887",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "741c505d",
   "metadata": {},
   "source": [
    "###  NOOO Combine num and cat selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7a1c35",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T09:18:09.155686Z",
     "start_time": "2023-07-23T09:18:09.150145Z"
    }
   },
   "outputs": [],
   "source": [
    "features_selected = [col for col in X_train.columns if col in all_st_fs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5514972d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T09:18:10.439342Z",
     "start_time": "2023-07-23T09:18:10.425347Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_fs, X_test_fs = X_train[features_selected], X_test[features_selected]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592cc160",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c16b81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c56628",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74df651b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "de4c4cba",
   "metadata": {},
   "source": [
    "## Balance the dependent variable of the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c763bb0",
   "metadata": {},
   "source": [
    "Multiple ways to do that:\n",
    "- SMOTE\n",
    "- **TODO** RandomOverSampler with correlation-aware sampling (ROS-CAS)\n",
    "\n",
    "\n",
    "Will split y_train into four different y_trains for each category. The idea is that we'll have four separate models in the end that are going to make predictions for each class separately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7821f430",
   "metadata": {},
   "source": [
    "### SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98f9b22",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T09:18:11.951103Z",
     "start_time": "2023-07-23T09:18:11.940588Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "y_train_overdraft = y_train[\"overdraft_YES\"]\n",
    "y_train_cons_cred = y_train[\"cons_cred_YES\"]\n",
    "y_train_mortgage  = y_train[\"mortgage_YES\"]\n",
    "y_train_bk_cc     = y_train[\"bk_cc_YES\"]\n",
    "\n",
    "\n",
    "\n",
    "y_test_overdraft = y_test[\"overdraft_YES\"]\n",
    "y_test_cons_cred = y_test[\"cons_cred_YES\"]\n",
    "y_test_mortgage  = y_test[\"mortgage_YES\"]\n",
    "y_test_bk_cc     = y_test[\"bk_cc_YES\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6ab59b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T09:18:13.266674Z",
     "start_time": "2023-07-23T09:18:13.251617Z"
    }
   },
   "outputs": [],
   "source": [
    "y_train_overdraft.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a826296",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T09:18:14.525739Z",
     "start_time": "2023-07-23T09:18:14.511207Z"
    }
   },
   "outputs": [],
   "source": [
    "def oversample_with_SMOTE(train_X, train_y):\n",
    "    \n",
    "\n",
    "    os = SMOTE(random_state=42)\n",
    "\n",
    "    os_X_tr, os_y_tr = os.fit_resample(train_X, train_y)\n",
    "    # TODO ...the rest to follow later\n",
    "\n",
    "    df_os_X_tr = pd.DataFrame(data=os_X_tr ,columns=train_X.columns)\n",
    "    df_os_y_tr = pd.DataFrame(data=os_y_tr, columns=[train_y.name])\n",
    "\n",
    "\n",
    "    # check old and new distributions:\n",
    "    print(\"Original data target distributions:\")\n",
    "    print(train_y.value_counts())\n",
    "    print()\n",
    "    print()\n",
    "    print(\"Oversampled data target distributions:\")\n",
    "    print(df_os_y_tr.value_counts())\n",
    "    \n",
    "    \n",
    "    return df_os_X_tr, df_os_y_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12295107",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T09:18:18.578689Z",
     "start_time": "2023-07-23T09:18:15.755797Z"
    }
   },
   "outputs": [],
   "source": [
    "df_os_X_train_overdraft, df_os_y_train_overdraft = oversample_with_SMOTE(train_X=X_train, train_y=y_train_overdraft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130c0de3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T09:18:20.030245Z",
     "start_time": "2023-07-23T09:18:20.015717Z"
    }
   },
   "outputs": [],
   "source": [
    "y_train_overdraft_ravel = df_os_y_train_overdraft.values.ravel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb08939",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T09:18:21.782926Z",
     "start_time": "2023-07-23T09:18:21.450918Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_fs_overdraft_OS, df_os_y_train_overdraft = oversample_with_SMOTE(train_X=X_train_fs, train_y=y_train_overdraft)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9a2b98",
   "metadata": {},
   "source": [
    "#### NOO Save training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758933d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T09:18:23.509024Z",
     "start_time": "2023-07-23T09:18:23.504026Z"
    }
   },
   "outputs": [],
   "source": [
    "# df_os_X_train_overdraft.to_csv(\"df_os_X_train_overdraft_v1.csv\")\n",
    "# pd.Series(y_train_overdraft_ravel, name='overdraft').to_csv(\"y_train_overdraft_ravel_v1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb3f7fb",
   "metadata": {},
   "source": [
    "# Modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d92143e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T09:18:25.030608Z",
     "start_time": "2023-07-23T09:18:25.015856Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, GridSearchCV, RepeatedStratifiedKFold\n",
    "from sklearn.metrics import make_scorer, recall_score, auc\n",
    "from sklearn.model_selection import cross_validate, KFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908a4a9f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T09:18:27.110101Z",
     "start_time": "2023-07-23T09:18:27.098071Z"
    }
   },
   "outputs": [],
   "source": [
    "def model_cv_performance_measure(model, X, y, cv):\n",
    "\n",
    "\n",
    "    # Define the scoring metric you want to use (in this case, recall)\n",
    "    scorer = make_scorer(recall_score, average='binary')\n",
    "    # scorer = make_scorer(auc)\n",
    "\n",
    "\n",
    "    # Perform cross-validation and compute recall\n",
    "    recall_scores = cross_val_score(model, X, y, cv=cv, scoring=scorer)\n",
    "\n",
    "    # The recall scores for each fold will be stored in recall_scores\n",
    "    print(\"Recall scores for each fold:\", recall_scores)\n",
    "\n",
    "    # The average recall score across all folds\n",
    "    print(\"Min recall:\", recall_scores.min())\n",
    "\n",
    "    model.fit(X, y)\n",
    "    print()\n",
    "    y_pred = model.predict(X)# performance\n",
    "    print(f'Accuracy Score: {accuracy_score(y,y_pred)}')\n",
    "    print(f'Confusion Matrix: \\n{confusion_matrix(y, y_pred)}')\n",
    "    print(f'Area Under Curve: {roc_auc_score(y, y_pred)}')\n",
    "    print(f'Recall score: {recall_score(y,y_pred)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6845ea1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T09:18:28.623616Z",
     "start_time": "2023-07-23T09:18:28.609102Z"
    }
   },
   "outputs": [],
   "source": [
    "def model_test_performance_measure(model, train_X, train_y, test_X, test_y, cv):\n",
    "    \n",
    "    model.fit(train_X, train_y)\n",
    "    print()\n",
    "    y_pred = model.predict(test_X)# performance\n",
    "    print(f'Accuracy Score: {accuracy_score(test_y,y_pred)}')\n",
    "    print(f'Confusion Matrix: \\n{confusion_matrix(test_y, y_pred)}')\n",
    "    print(f'Area Under Curve: {roc_auc_score(test_y, y_pred)}')\n",
    "    print(f'Recall score: {recall_score(test_y,y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcd4d88",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T09:18:30.074711Z",
     "start_time": "2023-07-23T09:18:30.059647Z"
    }
   },
   "outputs": [],
   "source": [
    "def test_model_cv_fit(model, train_X, train_y, test_X, test_y, cv, model_idx):\n",
    "    \n",
    "    from sklearn.model_selection import cross_validate, KFold\n",
    "\n",
    "    # Define the scoring metric you want to use (in this case, recall)\n",
    "    scorer = make_scorer(recall_score, average='binary')\n",
    "\n",
    "    # Perform cross-validation with the custom scorer\n",
    "    cv_results = cross_validate(estimator=model, \n",
    "                                X=df_os_X_train_overdraft,\n",
    "                                y=y_train_overdraft_ravel,\n",
    "                                cv=5, \n",
    "                                scoring=scorer,\n",
    "                                return_estimator=True,\n",
    "                                return_indices=True,\n",
    "                                return_train_score=True)\n",
    "    \n",
    "    \n",
    "    print(f\"cv_results train folds: {cv_results['train_score']}\")\n",
    "    print(f\"cv_results test folds: {cv_results['test_score']}\")\n",
    "    train_indices = cv_results[\"indices\"][\"train\"][model_idx]\n",
    "\n",
    "\n",
    "    best_clf = cv_results[\"estimator\"][model_idx]\n",
    "    best_clf.fit(train_X.loc[train_indices], pd.DataFrame(train_y).loc[train_indices])\n",
    "\n",
    "    y_pred_train = best_clf.predict(train_X)# performance\n",
    "    print(f'Training statistics')\n",
    "    print(f'Accuracy Score: {accuracy_score(train_y,y_pred_train)}')\n",
    "    print(f'Confusion Matrix: \\n{confusion_matrix(train_y, y_pred_train)}')\n",
    "    print(f'Area Under Curve: {roc_auc_score(train_y, y_pred_train)}')\n",
    "    print(f'Recall score: {recall_score(train_y,y_pred_train)}')\n",
    "    print()\n",
    "    \n",
    "    y_pred_test = best_clf.predict(test_X)# performance\n",
    "    print(f'Test statistics')\n",
    "    print(f'Accuracy Score: {accuracy_score(test_y,y_pred_test)}')\n",
    "    print(f'Confusion Matrix: \\n{confusion_matrix(test_y, y_pred_test)}')\n",
    "    print(f'Area Under Curve: {roc_auc_score(test_y, y_pred_test)}')\n",
    "    print(f'Recall score: {recall_score(test_y,y_pred_test)}')   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11004950",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T09:18:31.734269Z",
     "start_time": "2023-07-23T09:18:31.719180Z"
    }
   },
   "outputs": [],
   "source": [
    "# def model_predict(model, train_X, train_y, test_X, test_y):\n",
    "    \n",
    "#     from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, recall_score\n",
    "    \n",
    "#     # extract values\n",
    "#     train_y_values = train_y.values.flatten() \n",
    "    \n",
    "#     # fit it\n",
    "#     model.fit(train_X, train_y_values)\n",
    "    \n",
    "#     # test\n",
    "#     pred_y = model.predict(test_X)# performance\n",
    "#     probas_y = model.predict_proba(test_X)[:, 1]\n",
    "#     print(f'Accuracy Score: {accuracy_score(test_y, pred_y)}')\n",
    "#     print(f'Confusion Matrix: \\n{confusion_matrix(test_y, pred_y)}')\n",
    "#     print(f'Area Under Curve: {roc_auc_score(test_y, pred_y)}')\n",
    "#     print(f'Recall score: {recall_score(test_y, pred_y)}')\n",
    "    \n",
    "#     return pred_y, probas_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13850164",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112df902",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T09:42:12.187268Z",
     "start_time": "2023-07-23T09:18:33.156798Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lg1 = LogisticRegression(random_state=42, max_iter=1000, class_weight={0: 0.22, 1: 0.78})\n",
    "\n",
    "for i in range(5):\n",
    "    \n",
    "    print(f\"model_fold: {i}\")\n",
    "    test_model_cv_fit(model=lg1,\n",
    "                     train_X=df_os_X_train_overdraft,\n",
    "                     train_y=y_train_overdraft_ravel,\n",
    "                     test_X=X_test,\n",
    "                     test_y=y_test_overdraft,\n",
    "                     cv=5,\n",
    "                     model_idx=i)\n",
    "    \n",
    "    print()\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efabe24d",
   "metadata": {},
   "source": [
    "### with Features Selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c792ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T10:01:55.191427Z",
     "start_time": "2023-07-23T09:42:13.655873Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lg1 = LogisticRegression(random_state=42, max_iter=1000, class_weight={0: 0.22, 1: 0.78})\n",
    "\n",
    "for i in range(5):\n",
    "    \n",
    "    print(f\"model_fold: {i}\")\n",
    "    test_model_cv_fit(model=lg1,\n",
    "                     train_X=X_train_fs_overdraft_OS,\n",
    "                     train_y=y_train_overdraft_ravel,\n",
    "                     test_X=X_test_fs,\n",
    "                     test_y=y_test_overdraft,\n",
    "                     cv=5,\n",
    "                     model_idx=i)\n",
    "    \n",
    "    print()\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2882fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T10:01:56.610820Z",
     "start_time": "2023-07-23T10:01:56.595786Z"
    }
   },
   "outputs": [],
   "source": [
    "# pred_y_overdraft, probas_y_overdraft = model_predict(model=lg1,\n",
    "#                                 train_X=df_os_X_train_overdraft,\n",
    "#                                 train_y=df_os_y_train_overdraft,\n",
    "#                                 test_X=X_test,\n",
    "#                                 test_y=y_test_overdraft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f16928",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T10:01:57.999561Z",
     "start_time": "2023-07-23T10:01:57.985212Z"
    }
   },
   "outputs": [],
   "source": [
    "y_train_overdraft.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b516959",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T10:01:59.385651Z",
     "start_time": "2023-07-23T10:01:59.370666Z"
    }
   },
   "outputs": [],
   "source": [
    "y_test_overdraft.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f3b075",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb51cd6",
   "metadata": {},
   "source": [
    "### Cross validation (extract the best model and fit/predict the data with it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b015c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T10:08:47.542942Z",
     "start_time": "2023-07-23T10:02:00.757654Z"
    }
   },
   "outputs": [],
   "source": [
    "cl_1 = 0.5\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, \n",
    "                                class_weight={0: 1-cl_1, 1: cl_1},\n",
    "                                max_depth=5, \n",
    "                                min_samples_leaf=100, \n",
    "                                max_leaf_nodes=20)\n",
    "for i in range(5):\n",
    "    \n",
    "    print(f\"model_fold: {i}\")\n",
    "    test_model_cv_fit(model=rf_clf,\n",
    "                     train_X=df_os_X_train_overdraft,\n",
    "                     train_y=y_train_overdraft_ravel,\n",
    "                     test_X=X_test,\n",
    "                     test_y=y_test_overdraft,\n",
    "                     cv=5,\n",
    "                     model_idx=i)\n",
    "    \n",
    "    print()\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b102d47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b23bfda6",
   "metadata": {},
   "source": [
    "### with Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f33a09d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T10:14:51.245573Z",
     "start_time": "2023-07-23T10:08:47.543943Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "cl_1 = 0.5\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, \n",
    "                                class_weight={0: 1-cl_1, 1: cl_1},\n",
    "                                max_depth=5, \n",
    "                                min_samples_leaf=100, \n",
    "                                max_leaf_nodes=20)\n",
    "for i in range(5):\n",
    "    \n",
    "    print(f\"model_fold: {i}\")\n",
    "    test_model_cv_fit(model=rf_clf,\n",
    "                     train_X=X_train_fs_overdraft_OS,\n",
    "                     train_y=y_train_overdraft_ravel,\n",
    "                     test_X=X_test_fs,\n",
    "                     test_y=y_test_overdraft,\n",
    "                     cv=5,\n",
    "                     model_idx=i)\n",
    "    \n",
    "    print()\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1a52dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a3b846a",
   "metadata": {},
   "source": [
    "- n_estimators: 100 (default)\n",
    "- max_depth: 5 (around 5) is ok\n",
    "- min_samples_leaf: min 100, could be much more (500)\n",
    "- max_leaf_nodes: around 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac6df29",
   "metadata": {},
   "source": [
    "TODO: cross validation vs fit; finish the pipeline - to have the models; each model to have a grid (not more than 3-4 params with 3-4 values); run the hypergrid; class weights??? download kaggle dataset, load it with this code and if the model is shit the model is the problem, otherwise the data is the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac218be",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a058cf2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T10:20:13.697283Z",
     "start_time": "2023-07-23T10:14:53.980243Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define model\n",
    "tree_clf = DecisionTreeClassifier()\n",
    "\n",
    "for i in range(5):\n",
    "    \n",
    "    print(f\"model_fold: {i}\")\n",
    "    test_model_cv_fit(model=tree_clf,\n",
    "                     train_X=df_os_X_train_overdraft,\n",
    "                     train_y=y_train_overdraft_ravel,\n",
    "                     test_X=X_test,\n",
    "                     test_y=y_test_overdraft,\n",
    "                     cv=5,\n",
    "                     model_idx=i)\n",
    "    \n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3303f122",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1582593c",
   "metadata": {},
   "source": [
    "### with Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd2202b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T10:24:39.134356Z",
     "start_time": "2023-07-23T10:20:14.800284Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tree_clf = DecisionTreeClassifier()\n",
    "\n",
    "for i in range(5):\n",
    "    \n",
    "    print(f\"model_fold: {i}\")\n",
    "    test_model_cv_fit(model=tree_clf,\n",
    "                     train_X=X_train_fs_overdraft_OS,\n",
    "                     train_y=y_train_overdraft_ravel,\n",
    "                     test_X=X_test_fs,\n",
    "                     test_y=y_test_overdraft,\n",
    "                     cv=5,\n",
    "                     model_idx=i)\n",
    "    \n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2545608",
   "metadata": {},
   "source": [
    "## XgBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6545eda",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T10:29:52.882221Z",
     "start_time": "2023-07-23T10:24:40.237357Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create model instance\n",
    "xgboost_clf = XGBClassifier()\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    \n",
    "    print(f\"model_fold: {i}\")\n",
    "    test_model_cv_fit(model=tree_clf,\n",
    "                     train_X=df_os_X_train_overdraft,\n",
    "                     train_y=y_train_overdraft_ravel,\n",
    "                     test_X=X_test,\n",
    "                     test_y=y_test_overdraft,\n",
    "                     cv=5,\n",
    "                     model_idx=i)\n",
    "    \n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3de6a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8817ff40",
   "metadata": {},
   "source": [
    "### with Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ea4c49",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T10:34:18.403729Z",
     "start_time": "2023-07-23T10:29:53.984221Z"
    }
   },
   "outputs": [],
   "source": [
    "# create model instance\n",
    "xgboost_clf = XGBClassifier()\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    \n",
    "    print(f\"model_fold: {i}\")\n",
    "    test_model_cv_fit(model=tree_clf,\n",
    "                     train_X=X_train_fs_overdraft_OS,\n",
    "                     train_y=y_train_overdraft_ravel,\n",
    "                     test_X=X_test_fs,\n",
    "                     test_y=y_test_overdraft,\n",
    "                     cv=5,\n",
    "                     model_idx=i)\n",
    "    \n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a86e85",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba898666",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T10:34:19.535315Z",
     "start_time": "2023-07-23T10:34:19.520278Z"
    }
   },
   "outputs": [],
   "source": [
    "# logistic_regression_search_space = {\n",
    "#     'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "#     'penalty': ['l1', 'l2'],\n",
    "#     'solver': ['liblinear', 'lbfgs', 'saga']\n",
    "# }\n",
    "\n",
    "# decision_tree_search_space = {\n",
    "#     'criterion': ['gini', 'entropy'],\n",
    "#     'max_depth': [None, 5, 10, 20, 30],\n",
    "#     'min_samples_split': [2, 5, 10, 20],\n",
    "#     'min_samples_leaf': [1, 2, 4, 8]\n",
    "# }\n",
    "\n",
    "# # random forest\n",
    "# #     n_estimators: 100 (default)\n",
    "# #     max_depth: 5 (around 5) is ok\n",
    "# #     min_samples_leaf: min 100, could be much more (500)\n",
    "# #     max_leaf_nodes: around 20\n",
    "\n",
    "# random_forest_search_space = {\n",
    "#     'n_estimators': [50, 100, 200, 300],\n",
    "#     'criterion': ['gini', 'entropy'],\n",
    "#     'max_depth': [None, 5, 7, 8, 10],\n",
    "#     'min_samples_leaf': [100, 200, 400, 500, 787],\n",
    "#     'max_leaf_nodes': [10, 20, 50, 100, 200]\n",
    "# }\n",
    "\n",
    "# xgboost_search_space = {\n",
    "#     'n_estimators': [50, 100, 500, 1000],\n",
    "#     'max_depth': [13],\n",
    "#     'min_child_weight': [1, 5, 10],\n",
    "#     'subsample': [0.8, 0.9, 1.0],\n",
    "#     'colsample_bytree': [0.8, 0.9, 1.0]\n",
    "# }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfc5cfb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T10:34:20.698419Z",
     "start_time": "2023-07-23T10:34:20.683883Z"
    }
   },
   "outputs": [],
   "source": [
    "# grid_results_dict = {}\n",
    "\n",
    "# def grid_search(X, y, models_list, space_list, scoring, n_jobs, cv, save_dict):\n",
    "    \n",
    "#     for model, space in zip(models_list, space_list):\n",
    "        \n",
    "#         model_name = str(model)\n",
    "\n",
    "#         search = GridSearchCV(model, space, scoring, n_jobs=n_jobs, cv=cv)\n",
    "#         # execute search\n",
    "#         result = search.fit(X, y)\n",
    "#         # summarize result\n",
    "#         print('Best Score: %s' % result.best_score_)\n",
    "#         print('Best Hyperparameters: %s' % result.best_params_)\n",
    "\n",
    "#         save_dict[model_name] = {'best_score_':result.best_score_,\n",
    "#                                 'best_params_':result.best_params_}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2481d567",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T10:34:21.845453Z",
     "start_time": "2023-07-23T10:34:21.830419Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# grid_search(X=df_os_X_train_overdraft,\n",
    "#            y=y_train_overdraft_ravel,\n",
    "#            models_list=[lg1, tree_clf, rf_clf, xgboost_clf],\n",
    "#            space_list=[logistic_regression_search_space, decision_tree_search_space, random_forest_search_space, xgboost_search_space],\n",
    "#            scoring='recall',\n",
    "#            n_jobs=-1,\n",
    "#            cv=5,\n",
    "#            save_dict=grid_results_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440f7f17",
   "metadata": {},
   "source": [
    "xgboost: \n",
    "- n_estimators: around 500\n",
    "- max_depth: 13 (square root of num of features)\n",
    "- subsample - ok"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc882970",
   "metadata": {},
   "source": [
    "Best Score: 0.6737725686809087\n",
    "Best Hyperparameters: {'C': 100, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
    "Best Score: 0.7464558296489499\n",
    "Best Hyperparameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
    "Best Score: 0.7891783938266397\n",
    "Best Hyperparameters: {'criterion': 'gini', 'max_depth': None, 'max_leaf_nodes': 200, 'min_samples_leaf': 100, 'n_estimators': 300}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdda90e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a08a18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3543b67c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f13966",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eac2857",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13623de3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b532a3bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "22090c0f",
   "metadata": {},
   "source": [
    "## Save training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050a6eee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be300a31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579e74f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe0e269",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0add7a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "592255fb",
   "metadata": {},
   "source": [
    "### Analyse feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3b7d71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7fe991f3",
   "metadata": {},
   "source": [
    "#### Feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d691ef0c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-03T17:10:10.197369Z",
     "start_time": "2022-09-03T17:10:10.183373Z"
    }
   },
   "outputs": [],
   "source": [
    "rf_clf.feature_importances_[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf58213",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-03T17:10:16.212147Z",
     "start_time": "2022-09-03T17:10:16.055608Z"
    }
   },
   "outputs": [],
   "source": [
    "# rf_feat_importances = pd.Series(rf_clf.feature_importances_, index=df_os_X_train_overdraft.columns)\n",
    "# rf_feat_importances.nlargest(10).plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133706b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-03T17:10:22.111238Z",
     "start_time": "2022-09-03T17:10:22.096199Z"
    }
   },
   "outputs": [],
   "source": [
    "# rf_best_feat_importances = list(rf_feat_importances.nlargest(10).index)\n",
    "# rf_best_feat_importances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b93b682",
   "metadata": {},
   "source": [
    "# Modeling after Hyperparam tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121c96ec",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e06ad7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T20:07:50.988375Z",
     "start_time": "2023-07-22T19:59:40.282967Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lg1 = LogisticRegression(C=100,\n",
    "                         penalty='l2',\n",
    "                         solver='lbfgs',\n",
    "                         random_state=42,\n",
    "                         max_iter=1000,\n",
    "                         class_weight={0: 0.22, 1: 0.78})\n",
    "\n",
    "for i in range(5):\n",
    "    \n",
    "    print(f\"model_fold: {i}\")\n",
    "    test_model_cv_fit(model=lg1,\n",
    "                     train_X=df_os_X_train_overdraft,\n",
    "                     train_y=y_train_overdraft_ravel,\n",
    "                     test_X=X_test,\n",
    "                     test_y=y_test_overdraft,\n",
    "                     cv=5,\n",
    "                     model_idx=i)\n",
    "    \n",
    "    print()\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09a8a8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "135b18dd",
   "metadata": {},
   "source": [
    "## Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9883e933",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T20:11:46.605198Z",
     "start_time": "2023-07-22T20:09:10.903192Z"
    }
   },
   "outputs": [],
   "source": [
    "# define model\n",
    "tree_clf = DecisionTreeClassifier(criterion='gini', \n",
    "                                  max_depth=None,\n",
    "                                  min_samples_leaf=2,\n",
    "                                  min_samples_split=2)\n",
    "\n",
    "for i in range(5):\n",
    "    \n",
    "    print(f\"model_fold: {i}\")\n",
    "    test_model_cv_fit(model=tree_clf,\n",
    "                     train_X=df_os_X_train_overdraft,\n",
    "                     train_y=y_train_overdraft_ravel,\n",
    "                     test_X=X_test,\n",
    "                     test_y=y_test_overdraft,\n",
    "                     cv=5,\n",
    "                     model_idx=i)\n",
    "    \n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cda6f88",
   "metadata": {},
   "source": [
    "## Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7614c98f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T20:49:49.883129Z",
     "start_time": "2023-07-22T20:12:25.861470Z"
    }
   },
   "outputs": [],
   "source": [
    "cl_1 = 0.5\n",
    "rf_clf = RandomForestClassifier(class_weight={0: 1-cl_1, 1: cl_1},\n",
    "                                criterion='gini',\n",
    "                                max_depth=None,\n",
    "                                max_leaf_nodes=200,\n",
    "                                min_samples_leaf=100,\n",
    "                                n_estimators=300)\n",
    "for i in range(5):\n",
    "    \n",
    "    print(f\"model_fold: {i}\")\n",
    "    test_model_cv_fit(model=rf_clf,\n",
    "                     train_X=df_os_X_train_overdraft,\n",
    "                     train_y=y_train_overdraft_ravel,\n",
    "                     test_X=X_test,\n",
    "                     test_y=y_test_overdraft,\n",
    "                     cv=5,\n",
    "                     model_idx=i)\n",
    "    \n",
    "    print()\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4675044e",
   "metadata": {},
   "source": [
    "## xgBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2f46ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T20:52:28.852205Z",
     "start_time": "2023-07-22T20:49:50.267432Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create model instance\n",
    "xgboost_clf = XGBClassifier(n_estimators=500,\n",
    "                            max_depth=13,\n",
    "                            subsample=0.8,\n",
    "                            colsample_bytree=0.9)\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    \n",
    "    print(f\"model_fold: {i}\")\n",
    "    test_model_cv_fit(model=tree_clf,\n",
    "                     train_X=df_os_X_train_overdraft,\n",
    "                     train_y=y_train_overdraft_ravel,\n",
    "                     test_X=X_test,\n",
    "                     test_y=y_test_overdraft,\n",
    "                     cv=5,\n",
    "                     model_idx=i)\n",
    "    \n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5a9ae6",
   "metadata": {},
   "source": [
    "# Conclusion and Next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb35c6d",
   "metadata": {},
   "source": [
    "**1. Data Generation**\n",
    "- We didn't generate our data based on the pers_exp column (which should've served as our base column for generating the rest of the numerical data)\n",
    "- We didn't manage to generate the data using Cholesky - either we had a good normal distributions with bad correlation matrices, or good correlation matrices with bad values (not complying with lower and upper boundaries, means, etc.)\n",
    "- A question remains whether the business rules are sufficient to guarantee proper logical relations in the data.\n",
    "- Apply business rules on an iterative approach (try different techniques to fill in the data)\n",
    "\n",
    "**2. Feature Engineering**\n",
    "- Wanted to try Genetic Algorithms for Feature Selection - didn't work out so far, but it's a TODO\n",
    "- Binning!\n",
    "- Check interaction effects between features (Dani)\n",
    "\n",
    "**3. Modeling**\n",
    "- run the models on another similar dataset \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f39f7c",
   "metadata": {},
   "source": [
    "# Run the models on another data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce6861a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba80f8fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bf792f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a734088",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c04397",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deeda43b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29969197",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "49bd398d",
   "metadata": {},
   "source": [
    "# Alternative tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd279e5",
   "metadata": {},
   "source": [
    "## Generate pers_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6912cd89",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-20T19:16:05.771852Z",
     "start_time": "2023-07-20T19:16:05.747049Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import truncnorm\n",
    "\n",
    "# Define the given distribution and its corresponding bins\n",
    "distribution = [0.10, 0.44, 0.33, 0.14]\n",
    "bins = [4500, 5000, 5500, 6000]\n",
    "\n",
    "# Define the desired mean and standard deviation\n",
    "mean = 5305.00\n",
    "std_dev = 500  # You can adjust this value to control the spread of the data\n",
    "\n",
    "# Generate random values following a truncated normal distribution\n",
    "size_of_data = 1000\n",
    "a, b = (bins[0] - mean) / std_dev, (bins[-1] - mean) / std_dev\n",
    "truncated_data = truncnorm.rvs(a, b, loc=mean, scale=std_dev, size=size_of_data)\n",
    "\n",
    "# Clip the data to fit within the specified bins\n",
    "pers_exp_data = np.clip(truncated_data, bins[0], bins[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b138704",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-20T19:16:06.628653Z",
     "start_time": "2023-07-20T19:16:06.251079Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Now you have the synthetic_data containing random values generated from the given distribution with the specified mean.\n",
    "# Plot the real-world data and the synthetic data\n",
    "plt.hist(pers_exp_data, bins=50, alpha=0.5, label='Synthetic data')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be6977b",
   "metadata": {},
   "source": [
    "## Generate the rest of the numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035c7e9e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-20T19:54:05.257742Z",
     "start_time": "2023-07-20T19:54:05.243704Z"
    }
   },
   "outputs": [],
   "source": [
    "def to_uniform(data):\n",
    "    # Convert data to uniform distribution using CDF\n",
    "    sorted_data = np.sort(data)\n",
    "    ranks = np.argsort(data)\n",
    "    u = (ranks - 0.5) / len(data)\n",
    "    return u\n",
    "\n",
    "def from_uniform(u, original_data):\n",
    "    sorted_data = np.sort(original_data)\n",
    "    ranks = (u * (len(original_data) - 1)).astype(int)\n",
    "    return np.interp(u, np.linspace(0, 1, len(original_data)), sorted_data)\n",
    "\n",
    "# Rest of the code remains the same\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfecf9fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-20T19:54:05.886882Z",
     "start_time": "2023-07-20T19:54:05.828945Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import truncnorm\n",
    "\n",
    "# Given correlation matrix\n",
    "corr = {\n",
    "    'features': ['age', 'ind_risk', 'income', 'pers_exp', 'house_exp', 'taxes', 'transp_telecom', 'hobby'],\n",
    "    'age': [1, -0.00665947056405372, 0.00291644965339247, 0.0107779942638097, 0.00698674581731255, 0.00729153655132963, 0.0099866509330216, 0.00931630696561133],\n",
    "    'ind_risk': [-0.00665947056405372, 1, 0.0039918072709289, 0.00806259039194059, 0.00457023635440603, 0.0061985340641631, 0.00768699810849585, -0.00332322616613201],\n",
    "    'income': [0.00291644965339247, 0.0039918072709289, 1, 0.560949334881676, 0.58892666343229, 0.581907424628933, 0.562946509689962, 0.352350802339294],\n",
    "    'pers_exp': [0.0107779942638097, 0.00806259039194059, 0.560949334881676, 1, 0.928449923861951, 0.929598634668897, 0.934775947642248, 0.714298364869941],\n",
    "    'house_exp': [0.00698674581731255, 0.00457023635440603, 0.58892666343229, 0.928449923861951, 1, 0.93031279279417, 0.927846735467478, 0.679286362990223],\n",
    "    'taxes': [0.00729153655132963, 0.0061985340641631, 0.581907424628933, 0.929598634668897, 0.93031279279417, 1, 0.92920510128812, 0.689442053350162],\n",
    "    'transp_telecom': [0.0099866509330216, 0.00768699810849585, 0.562946509689962, 0.934775947642248, 0.927846735467478, 0.92920510128812, 1, 0.714114127908189],\n",
    "    'hobby': [0.00931630696561133, -0.00332322616613201, 0.352350802339294, 0.714298364869941, 0.679286362990223, 0.689442053350162, 0.714114127908189, 1]\n",
    "}\n",
    "\n",
    "# DataFrame of means\n",
    "means = pd.DataFrame([50.58, 0.6039, 13579.30, 5305.00, 2260, 1500, 1335, 1740],\n",
    "                     columns=['mean'])\n",
    "means.index = ['age', 'ind_risk', 'income', 'pers_exp', 'house_exp', 'taxes', 'transp_telecom', 'hobby']\n",
    "\n",
    "# Define the given distribution and its corresponding bins for pers_exp\n",
    "distribution = [0.10, 0.44, 0.33, 0.14]\n",
    "bins = [4500, 5000, 5500, 6000]\n",
    "mean = 5305.00\n",
    "std_dev = 500\n",
    "size_of_data = 1000\n",
    "a, b = (bins[0] - mean) / std_dev, (bins[-1] - mean) / std_dev\n",
    "\n",
    "# Generate random values following a truncated normal distribution\n",
    "truncated_data = truncnorm.rvs(a, b, loc=mean, scale=std_dev, size=size_of_data)\n",
    "\n",
    "# Clip the data to fit within the specified bins\n",
    "pers_exp_data = np.clip(truncated_data, bins[0], bins[-1])\n",
    "\n",
    "# Transform \"pers_exp_data\" to uniform distribution\n",
    "u_pers_exp = to_uniform(pers_exp_data)\n",
    "\n",
    "# Generate random uncorrelated multivariate normal data\n",
    "random_data = np.random.normal(size=(size_of_data, len(means)))\n",
    "\n",
    "# Use eigendecomposition to introduce the desired correlation matrix\n",
    "corr_matrix = pd.DataFrame.from_dict(corr).drop('features', axis=1).values\n",
    "eigenvalues, eigenvectors = np.linalg.eig(corr_matrix)\n",
    "correlated_data = random_data @ np.sqrt(np.diag(eigenvalues)) @ eigenvectors.T\n",
    "\n",
    "# Scale and shift the data to match the desired mean and standard deviation\n",
    "scaled_data = correlated_data * std_dev + mean\n",
    "\n",
    "# Apply inverse transformation to the original distribution for all features\n",
    "synthetic_data = pd.DataFrame(data=from_uniform(scaled_data, u_pers_exp),\n",
    "                              columns=means.index)\n",
    "\n",
    "print(synthetic_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7027e48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import truncnorm\n",
    "\n",
    "# Function to generate age data\n",
    "def generate_age_data(size_of_data):\n",
    "    return np.random.randint(20, 86, size=size_of_data)\n",
    "\n",
    "# Function to generate individual risk preference data\n",
    "def generate_ind_risk_data(size_of_data):\n",
    "    return np.random.rand(size_of_data)\n",
    "\n",
    "# Function to generate income data\n",
    "def generate_income_data(size_of_data):\n",
    "    income_bins = [0, 6121, 12001, 27601, 43201, 58801, 74401, np.inf]\n",
    "    return np.random.choice(income_bins[:-1], size=size_of_data) + np.random.rand(size_of_data) * (income_bins[1] - income_bins[0])\n",
    "\n",
    "# Function to generate personal expenses data\n",
    "def generate_pers_exp_data(size_of_data):\n",
    "    pers_exp_bins = [0, 4500, 5000, 5500, 6000]\n",
    "    return np.random.choice(pers_exp_bins[:-1], size=size_of_data) + np.random.rand(size_of_data) * (pers_exp_bins[1] - pers_exp_bins[0])\n",
    "\n",
    "# Function to generate housing costs data\n",
    "def generate_house_exp_data(size_of_data):\n",
    "    house_exp_bins = [0, 500, 1500, 3000, 4000]\n",
    "    return np.random.choice(house_exp_bins[:-1], size=size_of_data) + np.random.rand(size_of_data) * (house_exp_bins[1] - house_exp_bins[0])\n",
    "\n",
    "# Function to generate taxes and insurance data\n",
    "def generate_taxes_data(size_of_data):\n",
    "    taxes_bins = [0, 500, 1000, 2000, 2500]\n",
    "    return np.random.choice(taxes_bins[:-1], size=size_of_data) + np.random.rand(size_of_data) * (taxes_bins[1] - taxes_bins[0])\n",
    "\n",
    "# Function to generate transport and communications data\n",
    "def generate_transp_telecom_data(size_of_data):\n",
    "    transp_telecom_bins = [0, 500, 1000, 1500, 2500]\n",
    "    return np.random.choice(transp_telecom_bins[:-1], size=size_of_data) + np.random.rand(size_of_data) * (transp_telecom_bins[1] - transp_telecom_bins[0])\n",
    "\n",
    "# Function to generate leisure and hobby data\n",
    "def generate_hobby_data(size_of_data):\n",
    "    hobby_bins = [0, 1500, 2000, 3000]\n",
    "    return np.random.choice(hobby_bins[:-1], size=size_of_data) + np.random.rand(size_of_data) * (hobby_bins[1] - hobby_bins[0])\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import truncnorm\n",
    "\n",
    "# ... (Same functions to generate data for each feature as provided in the previous response)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import truncnorm\n",
    "\n",
    "# ... (Same functions to generate data for each feature as provided in the previous response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c08a44c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-20T20:13:56.437992Z",
     "start_time": "2023-07-20T20:13:56.392308Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import truncnorm\n",
    "\n",
    "# ... (Same functions to generate data for each feature as provided in the previous response)\n",
    "\n",
    "# DataFrame of means\n",
    "means = pd.DataFrame([50.58, 0.6039, 13579.30, 5305.00, 2260, 1500, 1335, 1740],\n",
    "                     columns=['mean'])\n",
    "means.index = ['age', 'ind_risk', 'income', 'pers_exp', 'house_exp', 'taxes', 'transp_telecom', 'hobby']\n",
    "\n",
    "# Define the given distribution and its corresponding bins for pers_exp\n",
    "distribution = [0.10, 0.44, 0.33, 0.14]\n",
    "bins = [4500, 5000, 5500, 6000]\n",
    "mean = 5305.00\n",
    "std_dev = 500\n",
    "size_of_data = 1000\n",
    "a, b = (bins[0] - mean) / std_dev, (bins[-1] - mean) / std_dev\n",
    "\n",
    "# Generate random values following a standard normal distribution\n",
    "random_data = np.random.normal(size=(size_of_data, len(means)))\n",
    "\n",
    "# Define the desired correlation matrix\n",
    "corr_matrix = np.array([\n",
    "    [1.0, 0.8, 0.3, 0.2, 0.1, 0.1, 0.05, 0.01],\n",
    "    [0.8, 1.0, 0.5, 0.3, 0.2, 0.1, 0.05, 0.01],\n",
    "    [0.3, 0.5, 1.0, 0.5, 0.3, 0.2, 0.1, 0.05],\n",
    "    [0.2, 0.3, 0.5, 1.0, 0.8, 0.5, 0.3, 0.2],\n",
    "    [0.1, 0.2, 0.3, 0.8, 1.0, 0.8, 0.5, 0.3],\n",
    "    [0.1, 0.1, 0.2, 0.5, 0.8, 1.0, 0.8, 0.5],\n",
    "    [0.05, 0.05, 0.1, 0.3, 0.5, 0.8, 1.0, 0.8],\n",
    "    [0.01, 0.01, 0.05, 0.2, 0.3, 0.5, 0.8, 1.0]\n",
    "])\n",
    "\n",
    "# Use Cholesky decomposition to introduce the desired correlation matrix\n",
    "chol_decomp = np.linalg.cholesky(corr_matrix)\n",
    "\n",
    "# Generate correlated data using Cholesky decomposition\n",
    "correlated_data = random_data @ chol_decomp.T\n",
    "\n",
    "# Apply inverse transformation to the original distribution for pers_exp\n",
    "pers_exp_data = generate_pers_exp_data(size_of_data)\n",
    "sorted_pers_exp = np.sort(pers_exp_data)\n",
    "ranks = np.clip(correlated_data[:, 3], 0, 1) * (len(sorted_pers_exp) - 1)\n",
    "ranks = ranks.astype(int)\n",
    "synthetic_pers_exp = sorted_pers_exp[ranks]\n",
    "\n",
    "# Apply inverse transformation to the original distribution for other features\n",
    "synthetic_data = pd.DataFrame()\n",
    "synthetic_data['age'] = from_uniform(correlated_data[:, 0], age_data)\n",
    "synthetic_data['ind_risk'] = from_uniform(correlated_data[:, 1], ind_risk_data)\n",
    "synthetic_data['income'] = from_uniform(correlated_data[:, 2], income_data)\n",
    "synthetic_data['pers_exp'] = synthetic_pers_exp\n",
    "synthetic_data['house_exp'] = from_uniform(correlated_data[:, 4], house_exp_data)\n",
    "synthetic_data['taxes'] = from_uniform(correlated_data[:, 5], taxes_data)\n",
    "synthetic_data['transp_telecom'] = from_uniform(correlated_data[:, 6], transp_telecom_data)\n",
    "synthetic_data['hobby'] = from_uniform(correlated_data[:, 7], hobby_data)\n",
    "\n",
    "print(synthetic_data.corr())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345b6140",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98724dd4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-20T20:32:10.013595Z",
     "start_time": "2023-07-20T20:32:09.929321Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from vinecopulib import VineCopula\n",
    "\n",
    "# Define the number of samples\n",
    "size_of_data = 1000\n",
    "\n",
    "# Define the desired correlation matrix\n",
    "corr_matrix = np.array([\n",
    "    [1.0, 0.8, 0.3, 0.2, 0.1, 0.1, 0.05, 0.01],\n",
    "    [0.8, 1.0, 0.5, 0.3, 0.2, 0.1, 0.05, 0.01],\n",
    "    [0.3, 0.5, 1.0, 0.5, 0.3, 0.2, 0.1, 0.05],\n",
    "    [0.2, 0.3, 0.5, 1.0, 0.8, 0.5, 0.3, 0.2],\n",
    "    [0.1, 0.2, 0.3, 0.8, 1.0, 0.8, 0.5, 0.3],\n",
    "    [0.1, 0.1, 0.2, 0.5, 0.8, 1.0, 0.8, 0.5],\n",
    "    [0.05, 0.05, 0.1, 0.3, 0.5, 0.8, 1.0, 0.8],\n",
    "    [0.01, 0.01, 0.05, 0.2, 0.3, 0.5, 0.8, 1.0]\n",
    "])\n",
    "\n",
    "# Define the means and standard deviations for each feature\n",
    "means = pd.Series({\n",
    "    'age': 52.0,\n",
    "    'ind_risk': 0.5,\n",
    "    'income': 45000.0,\n",
    "    'pers_exp': 3500.0,\n",
    "    'house_exp': 2500.0,\n",
    "    'taxes': 1500.0,\n",
    "    'transp_telecom': 1250.0,\n",
    "    'hobby': 750.0\n",
    "})\n",
    "\n",
    "std_devs = pd.Series({\n",
    "    'age': 10.0,\n",
    "    'ind_risk': 0.2,\n",
    "    'income': 15000.0,\n",
    "    'pers_exp': 500.0,\n",
    "    'house_exp': 500.0,\n",
    "    'taxes': 200.0,\n",
    "    'transp_telecom': 200.0,\n",
    "    'hobby': 300.0\n",
    "})\n",
    "\n",
    "# Generate uniform data for VineCopula\n",
    "uniform_data = np.random.rand(size_of_data, len(means))\n",
    "\n",
    "# Apply inverse transformation to map the uniform data to the desired distributions\n",
    "for column_name in means.index:\n",
    "    bins = np.linspace(means[column_name] - 3 * std_devs[column_name],\n",
    "                       means[column_name] + 3 * std_devs[column_name], 100)\n",
    "    synthetic_data[column_name] = pd.cut(uniform_data[:, i], bins, labels=bins[:-1]).astype(float)\n",
    "\n",
    "# Calculate the correlation matrix of the synthetic data\n",
    "corr_matrix_synthetic = synthetic_data.corr()\n",
    "\n",
    "# Use VineCopula to capture the desired correlation structure\n",
    "copula = VineCopula(copula_order=2, family_set=[0, 1, 2, 3])\n",
    "copula.fit(synthetic_data)\n",
    "\n",
    "# Generate correlated data using VineCopula\n",
    "correlated_data = copula.sample(size_of_data)\n",
    "\n",
    "# Scale and shift the data to match the desired mean and standard deviation\n",
    "scaled_data = correlated_data * std_devs.values + means.values\n",
    "\n",
    "# Create the final synthetic dataset\n",
    "synthetic_data = pd.DataFrame(data=scaled_data, columns=means.index)\n",
    "\n",
    "# Print the correlation matrix of the synthetic data\n",
    "print(synthetic_data.corr())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b0606f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08657ae8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c79e64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6123a74f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fb8b74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7eec884",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-20T20:29:07.914718Z",
     "start_time": "2023-07-20T20:29:07.168350Z"
    }
   },
   "outputs": [],
   "source": [
    "for col in synthetic_data.columns:\n",
    "    plot_distribution(synthetic_data, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5056cd26",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-20T20:29:09.526850Z",
     "start_time": "2023-07-20T20:29:09.026418Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ... (Code from the previous answer)\n",
    "\n",
    "# Calculate the correlation matrix for the adjusted DataFrame\n",
    "correlation_matrix_adjusted = synthetic_data.corr()\n",
    "\n",
    "\n",
    "# Display the correlation matrix\n",
    "print(\"Adjusted Correlation Matrix:\")\n",
    "print(correlation_matrix_adjusted)\n",
    "\n",
    "# Convert the original correlation data (corr) to a DataFrame\n",
    "original_corr_df = pd.DataFrame(corr)\n",
    "original_corr_df.set_index('features', inplace=True)\n",
    "\n",
    "# Display the original correlation data\n",
    "print(\"\\nOriginal Correlation Matrix:\")\n",
    "print(original_corr_df)\n",
    "\n",
    "# Plot the correlation matrix heatmaps for both adjusted and original data side by side\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.suptitle(\"Comparison of Correlation Matrices\", fontsize=16)\n",
    "\n",
    "# Adjusted correlation matrix heatmap\n",
    "axes[0].imshow(correlation_matrix_adjusted, cmap='coolwarm', interpolation='nearest')\n",
    "axes[0].set_xticks(np.arange(len(correlation_matrix_adjusted)))\n",
    "axes[0].set_yticks(np.arange(len(correlation_matrix_adjusted)))\n",
    "axes[0].set_xticklabels(correlation_matrix_adjusted.columns, rotation=45)\n",
    "axes[0].set_yticklabels(correlation_matrix_adjusted.columns)\n",
    "axes[0].set_title(\"Adjusted Correlation Matrix\")\n",
    "\n",
    "# Original correlation matrix heatmap\n",
    "axes[1].imshow(original_corr_df, cmap='coolwarm', interpolation='nearest')\n",
    "axes[1].set_xticks(np.arange(len(original_corr_df)))\n",
    "axes[1].set_yticks(np.arange(len(original_corr_df)))\n",
    "axes[1].set_xticklabels(original_corr_df.columns, rotation=45)\n",
    "axes[1].set_yticklabels(original_corr_df.columns)\n",
    "axes[1].set_title(\"Original Correlation Matrix\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d9736e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908a82e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73115e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48afd847",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-20T19:16:13.092275Z",
     "start_time": "2023-07-20T19:16:13.066081Z"
    }
   },
   "outputs": [],
   "source": [
    "corr = {\n",
    "    'features': ['age', 'ind_risk', 'income', 'pers_exp', 'house_exp', 'taxes', 'transp_telecom', 'hobby'],\n",
    "    'age': [1, -0.00665947056405372, 0.00291644965339247, 0.0107779942638097, 0.00698674581731255, 0.00729153655132963, 0.0099866509330216, 0.00931630696561133],\n",
    "    'ind_risk': [-0.00665947056405372, 1, 0.0039918072709289, 0.00806259039194059, 0.00457023635440603, 0.0061985340641631, 0.00768699810849585, -0.00332322616613201],\n",
    "    'income': [0.00291644965339247, 0.0039918072709289, 1, 0.560949334881676, 0.58892666343229, 0.581907424628933, 0.562946509689962, 0.352350802339294],\n",
    "    'pers_exp': [0.0107779942638097, 0.00806259039194059, 0.560949334881676, 1, 0.928449923861951, 0.929598634668897, 0.934775947642248, 0.714298364869941],\n",
    "    'house_exp': [0.00698674581731255, 0.00457023635440603, 0.58892666343229, 0.928449923861951, 1, 0.93031279279417, 0.927846735467478, 0.679286362990223],\n",
    "    'taxes': [0.00729153655132963, 0.0061985340641631, 0.581907424628933, 0.929598634668897, 0.93031279279417, 1, 0.92920510128812, 0.689442053350162],\n",
    "    'transp_telecom': [0.0099866509330216, 0.00768699810849585, 0.562946509689962, 0.934775947642248, 0.927846735467478, 0.92920510128812, 1, 0.714114127908189],\n",
    "    'hobby': [0.00931630696561133, -0.00332322616613201, 0.352350802339294, 0.714298364869941, 0.679286362990223, 0.689442053350162, 0.714114127908189, 1]\n",
    "}\n",
    "\n",
    "l_corr_matrix = []\n",
    "\n",
    "for k, v in corr.items():\n",
    "    if k == \"features\":\n",
    "        continue\n",
    "    else:\n",
    "        l_corr_matrix.append(v)\n",
    "\n",
    "        \n",
    "arr_corr_matrix = np.array(l_corr_matrix)\n",
    "\n",
    "mean = [50.58, 0.6039, 13579.30, 5305.00, 2260, 1500, 1335, 1740]\n",
    "num_samples = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8de97b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-20T19:24:43.836911Z",
     "start_time": "2023-07-20T19:24:43.822907Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_correlated_random_variables(mean, covariance, num_samples):\n",
    "    L = np.linalg.cholesky(covariance)\n",
    "    normal_samples = np.random.normal(size=(num_samples, covariance.shape[0]))\n",
    "    return mean + np.dot(normal_samples, L.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df05ab44",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-20T19:24:44.582771Z",
     "start_time": "2023-07-20T19:24:44.533402Z"
    }
   },
   "outputs": [],
   "source": [
    "samples = generate_correlated_random_variables(mean, arr_corr_matrix, num_samples, pers_exp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4335ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-20T19:27:05.569041Z",
     "start_time": "2023-07-20T19:27:05.554514Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_correlated_random_variables(mean, covariance, num_samples, pers_exp):\n",
    "    # Assuming arr_corr_matrix is the covariance matrix with shape (1000, 1000)\n",
    "    # Assuming mean is the mean array with shape (1000,)\n",
    "\n",
    "    # Reshape pers_exp from (1000,) to (1000, 1)\n",
    "    reshaped_array = pers_exp.reshape(-1, 1)\n",
    "\n",
    "    # Assuming L is your covariance matrix with shape (8, 8)\n",
    "    L = covariance[:8, :8]  # Extract the first 8 rows and columns\n",
    "\n",
    "    # Perform matrix multiplication\n",
    "    result = mean + np.dot(reshaped_array, L.T).flatten()\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7a231d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-20T19:27:06.512639Z",
     "start_time": "2023-07-20T19:27:06.462419Z"
    }
   },
   "outputs": [],
   "source": [
    "samples = generate_correlated_random_variables(mean, arr_corr_matrix, num_samples, pers_exp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04e3bb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57036139",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-20T20:00:59.936863Z",
     "start_time": "2023-07-20T20:00:59.927863Z"
    }
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "def plot_distribution(data, column_name):\n",
    "    \"\"\"\n",
    "    Plots the distribution of a pandas column using Plotly.\n",
    "\n",
    "    Parameters:\n",
    "        data (pd.DataFrame): The pandas DataFrame containing the data.\n",
    "        column_name (str): The name of the column to plot.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Ensure the column exists in the DataFrame\n",
    "    if column_name not in data.columns:\n",
    "        raise ValueError(f\"Column '{column_name}' not found in the DataFrame.\")\n",
    "\n",
    "    # Use Plotly Express to plot the distribution\n",
    "    fig = px.histogram(data, x=column_name, nbins=50, title=f'Distribution of {column_name}')\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805d91e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-20T20:01:02.121792Z",
     "start_time": "2023-07-20T20:01:01.366500Z"
    }
   },
   "outputs": [],
   "source": [
    "for col in synthetic_df.columns:\n",
    "    plot_distribution(synthetic_df, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aaf42f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-20T19:54:46.512242Z",
     "start_time": "2023-07-20T19:54:46.021300Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ... (Code from the previous answer)\n",
    "\n",
    "# Calculate the correlation matrix for the adjusted DataFrame\n",
    "correlation_matrix_adjusted = synthetic_df.corr()\n",
    "\n",
    "\n",
    "# Display the correlation matrix\n",
    "print(\"Adjusted Correlation Matrix:\")\n",
    "print(correlation_matrix_adjusted)\n",
    "\n",
    "# Convert the original correlation data (corr) to a DataFrame\n",
    "original_corr_df = pd.DataFrame(corr)\n",
    "original_corr_df.set_index('features', inplace=True)\n",
    "\n",
    "# Display the original correlation data\n",
    "print(\"\\nOriginal Correlation Matrix:\")\n",
    "print(original_corr_df)\n",
    "\n",
    "# Plot the correlation matrix heatmaps for both adjusted and original data side by side\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.suptitle(\"Comparison of Correlation Matrices\", fontsize=16)\n",
    "\n",
    "# Adjusted correlation matrix heatmap\n",
    "axes[0].imshow(correlation_matrix_adjusted, cmap='coolwarm', interpolation='nearest')\n",
    "axes[0].set_xticks(np.arange(len(correlation_matrix_adjusted)))\n",
    "axes[0].set_yticks(np.arange(len(correlation_matrix_adjusted)))\n",
    "axes[0].set_xticklabels(correlation_matrix_adjusted.columns, rotation=45)\n",
    "axes[0].set_yticklabels(correlation_matrix_adjusted.columns)\n",
    "axes[0].set_title(\"Adjusted Correlation Matrix\")\n",
    "\n",
    "# Original correlation matrix heatmap\n",
    "axes[1].imshow(original_corr_df, cmap='coolwarm', interpolation='nearest')\n",
    "axes[1].set_xticks(np.arange(len(original_corr_df)))\n",
    "axes[1].set_yticks(np.arange(len(original_corr_df)))\n",
    "axes[1].set_xticklabels(original_corr_df.columns, rotation=45)\n",
    "axes[1].set_yticklabels(original_corr_df.columns)\n",
    "axes[1].set_title(\"Original Correlation Matrix\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5303eeeb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-20T19:54:47.250680Z",
     "start_time": "2023-07-20T19:54:47.229895Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to measure the distance between two correlation matrices using Frobenius norm\n",
    "def correlation_distance(matrix1, matrix2):\n",
    "    return np.linalg.norm(matrix1 - matrix2, ord='fro')\n",
    "\n",
    "\n",
    "current_distance = correlation_distance(matrix1=correlation_matrix_adjusted, matrix2=original_corr_df)\n",
    "current_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33ca972",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea0e633",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ed4bdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a890c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abde820",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385a0115",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c377f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390198af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d50840",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245567d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f761f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b27852d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8044e19c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22354963",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e035d594",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c616bff7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2e0a1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4202a9ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20bc97e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7aabd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b17edc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-20T18:46:04.955006Z",
     "start_time": "2023-07-20T18:46:04.887401Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the real-world data\n",
    "mu = 5305.00\n",
    "sigma = 700\n",
    "# sigma = [0.10,0.44,0.33,0.14]\n",
    "# Fit a probability distribution to the real-world data\n",
    "\n",
    "\n",
    "distribution = np.random.normal(mu, sigma, 1000)\n",
    "\n",
    "# Generate synthetic data using Monte Carlo simulation\n",
    "synthetic_data_MC = np.random.choice(distribution, size=100)\n",
    "\n",
    "# Plot the real-world data and the synthetic data\n",
    "plt.hist(synthetic_data_MC, bins=50, alpha=0.5, label='Synthetic data')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# TODO: I have to bin it afterwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2787e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b57488",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-20T18:37:12.494077Z",
     "start_time": "2023-07-20T18:37:12.128834Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_pers_exp_data(size):\n",
    "    # Define the mean and standard deviation for the normal distribution\n",
    "    mean = 5305.00\n",
    "    std_dev = 600  # You can adjust this value to control the spread of the data\n",
    "\n",
    "    # Generate random data following the normal distribution\n",
    "    data = np.random.normal(loc=mean, scale=std_dev, size=size)\n",
    "\n",
    "    # Map the data to the specified bins\n",
    "    bins = [0.10, 0.44, 0.33, 0.14]\n",
    "    bin_edges = [4500, 5000, 5500, 6000]\n",
    "    data = np.digitize(data, bin_edges, right=True) - 1\n",
    "\n",
    "    # Adjust values to fit within each bin\n",
    "    for i in range(len(bins)):\n",
    "        idx = (data == i)\n",
    "        data[idx] = np.clip(data[idx], 0, bin_edges[i] - 1) / 100.0\n",
    "\n",
    "    return data\n",
    "\n",
    "# Set the desired size of the synthetic data\n",
    "size_of_data = 1000\n",
    "\n",
    "# Generate the synthetic data\n",
    "synthetic_data = generate_pers_exp_data(size_of_data)\n",
    "\n",
    "# Plot the histogram to visualize the distribution\n",
    "plt.hist(synthetic_data, bins=50, edgecolor='black')\n",
    "plt.xlabel('pers_exp')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Synthetic pers_exp Data')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6fdf29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841e9086",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544d67a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-20T14:06:34.773588Z",
     "start_time": "2023-07-20T14:06:34.754063Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "corr = {\n",
    "    'features': ['age', 'ind_risk', 'income', 'pers_exp', 'house_exp', 'taxes', 'transp_telecom', 'hobby'],\n",
    "    'age': [1, -0.00665947056405372, 0.00291644965339247, 0.0107779942638097, 0.00698674581731255, 0.00729153655132963, 0.0099866509330216, 0.00931630696561133],\n",
    "    'ind_risk': [-0.00665947056405372, 1, 0.0039918072709289, 0.00806259039194059, 0.00457023635440603, 0.0061985340641631, 0.00768699810849585, -0.00332322616613201],\n",
    "    'income': [0.00291644965339247, 0.0039918072709289, 1, 0.560949334881676, 0.58892666343229, 0.581907424628933, 0.562946509689962, 0.352350802339294],\n",
    "    'pers_exp': [0.0107779942638097, 0.00806259039194059, 0.560949334881676, 1, 0.928449923861951, 0.929598634668897, 0.934775947642248, 0.714298364869941],\n",
    "    'house_exp': [0.00698674581731255, 0.00457023635440603, 0.58892666343229, 0.928449923861951, 1, 0.93031279279417, 0.927846735467478, 0.679286362990223],\n",
    "    'taxes': [0.00729153655132963, 0.0061985340641631, 0.581907424628933, 0.929598634668897, 0.93031279279417, 1, 0.92920510128812, 0.689442053350162],\n",
    "    'transp_telecom': [0.0099866509330216, 0.00768699810849585, 0.562946509689962, 0.934775947642248, 0.927846735467478, 0.92920510128812, 1, 0.714114127908189],\n",
    "    'hobby': [0.00931630696561133, -0.00332322616613201, 0.352350802339294, 0.714298364869941, 0.679286362990223, 0.689442053350162, 0.714114127908189, 1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0928b1f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-20T14:06:34.931434Z",
     "start_time": "2023-07-20T14:06:34.919850Z"
    }
   },
   "outputs": [],
   "source": [
    "l_corr_matrix = []\n",
    "\n",
    "for k, v in corr.items():\n",
    "    if k == \"features\":\n",
    "        continue\n",
    "    else:\n",
    "        l_corr_matrix.append(v)\n",
    "\n",
    "        \n",
    "arr_corr_matrix = np.array(l_corr_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d20dd75",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-20T14:07:57.986000Z",
     "start_time": "2023-07-20T14:07:57.978692Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_correlated_random_variables(mean, covariance, num_samples):\n",
    "    L = np.linalg.cholesky(covariance)\n",
    "    \n",
    "    # TO BE REPLACED BY DATA GENERATED FROM THE OLD FUNCTION THAT RESPECTS THE BOUNDARIES (generate_synthetic_data_with_bounds)\n",
    "    normal_samples = np.random.normal(size=(num_samples, covariance.shape[0]))\n",
    "    print(\"normal_samples shape\", normal_samples.shape)\n",
    "    return mean + np.dot(normal_samples, L.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cbb7b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-20T14:08:01.570209Z",
     "start_time": "2023-07-20T14:07:58.761542Z"
    }
   },
   "outputs": [],
   "source": [
    "n_samples = 10000\n",
    "\n",
    "samples = generate_correlated_random_variables(mean=means,\n",
    "                                               covariance=arr_corr_matrix,\n",
    "                                               num_samples=n_samples)\n",
    "df_test = pd.DataFrame(samples, columns=possible_values.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b3df93",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-20T14:06:49.100004Z",
     "start_time": "2023-07-20T14:06:49.004542Z"
    }
   },
   "outputs": [],
   "source": [
    "df_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1900594f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9cdaa0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T22:01:53.800801Z",
     "start_time": "2023-07-19T22:01:53.782778Z"
    }
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from scipy.stats import norm\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "# def generate_synthetic_data_with_bounds(correlation_matrix, num_samples, feature_bounds):\n",
    "#     num_features = correlation_matrix.shape[0]\n",
    "#     lower_bounds, upper_bounds = zip(*feature_bounds)\n",
    "#     scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    \n",
    "#     # Check if the correlation matrix is valid (symmetric and positive definite)\n",
    "#     if not np.allclose(correlation_matrix, correlation_matrix.T):\n",
    "#         raise ValueError(\"Correlation matrix must be symmetric.\")\n",
    "#     if not np.all(np.linalg.eigvals(correlation_matrix) > 0):\n",
    "#         raise ValueError(\"Correlation matrix must be positive definite.\")\n",
    "    \n",
    "#     # Generate synthetic data using multivariate normal distribution\n",
    "#     mean = np.zeros(num_features)\n",
    "#     synthetic_data = np.random.multivariate_normal(mean, correlation_matrix, num_samples)\n",
    "    \n",
    "#     # Apply Gaussian copula to maintain correlation structure\n",
    "#     synthetic_data = norm.cdf(synthetic_data)\n",
    "    \n",
    "#     # Scale the data to the specified bounds for each feature\n",
    "#     for i in range(num_features):\n",
    "#         synthetic_data[:, i] = lower_bounds[i] + synthetic_data[:, i] * (upper_bounds[i] - lower_bounds[i])\n",
    "    \n",
    "#     return synthetic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1eae1ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T22:02:45.865530Z",
     "start_time": "2023-07-19T22:02:45.848140Z"
    }
   },
   "outputs": [],
   "source": [
    "# feature_bounds = []\n",
    "\n",
    "# possible_values = {\n",
    "#     'age': [20, 86],\n",
    "#     'ind_risk': [0, 1],\n",
    "#     'income': [0, 150000],\n",
    "#     'pers_exp': [0, 6000],\n",
    "#     'house_exp': [0, 4000],\n",
    "#     'taxes': [0, 2500],\n",
    "#     'transp_telecom': [0, 2500],\n",
    "#     'hobby': [0, 3000],\n",
    "# }\n",
    "\n",
    "\n",
    "# for v in possible_values.values():\n",
    "    \n",
    "#     feature_bounds.append(tuple(v))\n",
    "\n",
    "# # Outcome of (generate_synthetic_data_with_bounds) goes as synthetic_samples in the following function\n",
    "\n",
    "# # Example usage:\n",
    "# correlation_matrix = np.array(l_corr_matrix)\n",
    "\n",
    "# num_samples = 10000\n",
    "\n",
    "# synthetic_samp = generate_synthetic_data_with_bounds(correlation_matrix=correlation_matrix, \n",
    "#                                                      num_samples=num_samples, \n",
    "#                                                      feature_bounds=feature_bounds)\n",
    "\n",
    "# mean = []\n",
    "\n",
    "# for k, v in possible_values.items():\n",
    "#     avg = sum(v)/len(v)\n",
    "#     mean.append(avg)  \n",
    "\n",
    "    \n",
    "# print(\"mean\", mean)\n",
    "\n",
    "# def generate_correlated_random_variables_V2(mean, \n",
    "#                                             covariance, \n",
    "#                                             synthetic_samples):\n",
    "    \n",
    "    \n",
    "#     L = np.linalg.cholesky(covariance)\n",
    "#     print(\"normal_samples shape\", synthetic_samples.shape)\n",
    "#     return mean + np.dot(synthetic_samples, L.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cec2c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T22:01:54.149485Z",
     "start_time": "2023-07-19T22:01:54.135485Z"
    }
   },
   "outputs": [],
   "source": [
    "# samples = generate_correlated_random_variables_V2(mean=mean, \n",
    "#                                             covariance=correlation_matrix, \n",
    "#                                             synthetic_samples=synthetic_samp)\n",
    "# df_test = pd.DataFrame(samples, columns=possible_values.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ad4412",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T21:16:38.511459Z",
     "start_time": "2023-07-19T21:16:38.038428Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c2b094",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T22:01:55.286646Z",
     "start_time": "2023-07-19T22:01:55.267118Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check the correlation of the generated variables\n",
    "correlation = df_test.corr()\n",
    "print(\"Correlation matrix:\")\n",
    "print(correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4e1ddf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T22:01:56.390467Z",
     "start_time": "2023-07-19T22:01:56.019467Z"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize the distribution of the generated variables\n",
    "sns.jointplot(x='age', y='income', data=df_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc5cad9",
   "metadata": {},
   "source": [
    "## Plot distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b50ad3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T22:02:00.595049Z",
     "start_time": "2023-07-19T22:02:00.580522Z"
    }
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "def plot_distribution(data, column_name):\n",
    "    \"\"\"\n",
    "    Plots the distribution of a pandas column using Plotly.\n",
    "\n",
    "    Parameters:\n",
    "        data (pd.DataFrame): The pandas DataFrame containing the data.\n",
    "        column_name (str): The name of the column to plot.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Ensure the column exists in the DataFrame\n",
    "    if column_name not in data.columns:\n",
    "        raise ValueError(f\"Column '{column_name}' not found in the DataFrame.\")\n",
    "\n",
    "    # Use Plotly Express to plot the distribution\n",
    "    fig = px.histogram(data, x=column_name, nbins=50, title=f'Distribution of {column_name}')\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6c2153",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T22:02:01.241313Z",
     "start_time": "2023-07-19T22:02:00.770314Z"
    }
   },
   "outputs": [],
   "source": [
    "for col in df_test.columns:\n",
    "    plot_distribution(df_test, col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb6db10",
   "metadata": {},
   "source": [
    "## Plot the two correlation matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e340a6c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T21:41:42.809584Z",
     "start_time": "2023-07-19T21:41:42.607008Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ... (Code from the previous answer)\n",
    "\n",
    "# Calculate the correlation matrix for the adjusted DataFrame\n",
    "correlation_matrix_adjusted = df_test.corr()\n",
    "\n",
    "\n",
    "# Display the correlation matrix\n",
    "print(\"Adjusted Correlation Matrix:\")\n",
    "print(correlation_matrix_adjusted)\n",
    "\n",
    "# Convert the original correlation data (corr) to a DataFrame\n",
    "original_corr_df = pd.DataFrame(corr)\n",
    "original_corr_df.set_index('features', inplace=True)\n",
    "\n",
    "# Display the original correlation data\n",
    "print(\"\\nOriginal Correlation Matrix:\")\n",
    "print(original_corr_df)\n",
    "\n",
    "# Plot the correlation matrix heatmaps for both adjusted and original data side by side\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.suptitle(\"Comparison of Correlation Matrices\", fontsize=16)\n",
    "\n",
    "# Adjusted correlation matrix heatmap\n",
    "axes[0].imshow(correlation_matrix_adjusted, cmap='coolwarm', interpolation='nearest')\n",
    "axes[0].set_xticks(np.arange(len(correlation_matrix_adjusted)))\n",
    "axes[0].set_yticks(np.arange(len(correlation_matrix_adjusted)))\n",
    "axes[0].set_xticklabels(correlation_matrix_adjusted.columns, rotation=45)\n",
    "axes[0].set_yticklabels(correlation_matrix_adjusted.columns)\n",
    "axes[0].set_title(\"Adjusted Correlation Matrix\")\n",
    "\n",
    "# Original correlation matrix heatmap\n",
    "axes[1].imshow(original_corr_df, cmap='coolwarm', interpolation='nearest')\n",
    "axes[1].set_xticks(np.arange(len(original_corr_df)))\n",
    "axes[1].set_yticks(np.arange(len(original_corr_df)))\n",
    "axes[1].set_xticklabels(original_corr_df.columns, rotation=45)\n",
    "axes[1].set_yticklabels(original_corr_df.columns)\n",
    "axes[1].set_title(\"Original Correlation Matrix\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6515889",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T21:41:54.931971Z",
     "start_time": "2023-07-19T21:41:54.923681Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to measure the distance between two correlation matrices using Frobenius norm\n",
    "def correlation_distance(matrix1, matrix2):\n",
    "    return np.linalg.norm(matrix1 - matrix2, ord='fro')\n",
    "\n",
    "\n",
    "current_distance = correlation_distance(matrix1=correlation_matrix_adjusted, matrix2=original_corr_df)\n",
    "current_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349f417e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4548a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "915f6ed2",
   "metadata": {},
   "source": [
    "## Run the distribution fitter to check the distributions of each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abde1f2a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T21:01:45.766919Z",
     "start_time": "2023-07-19T21:01:45.727893Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Define the inputs and their probability distributions\n",
    "inputs = {\"input_1\": {\"mean\": 10, \"std_dev\": 2},\n",
    "          \"input_2\": {\"mean\": 20, \"std_dev\": 3}}\n",
    "\n",
    "# Number of iterations\n",
    "num_iterations = 10000\n",
    "\n",
    "# Storage for simulation results\n",
    "results = []\n",
    "\n",
    "# Run the simulation\n",
    "for i in range(num_iterations):\n",
    "    # Generate random values for each input based on its distribution\n",
    "    input_1 = np.random.normal(inputs[\"input_1\"][\"mean\"], inputs[\"input_1\"][\"std_dev\"])\n",
    "    input_2 = np.random.normal(inputs[\"input_2\"][\"mean\"], inputs[\"input_2\"][\"std_dev\"])\n",
    "\n",
    "    # Calculate the output of the model\n",
    "    output = input_1**2 +input_1*input_2+ input_2**2\n",
    "\n",
    "    # Store the result\n",
    "    results.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96b1d48",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T21:02:47.253618Z",
     "start_time": "2023-07-19T21:02:46.931632Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Analyze the results\n",
    "mean_result = np.mean(results)\n",
    "std_dev_result = np.std(results)\n",
    "\n",
    "# Visualize the results\n",
    "sns.set_style('white')\n",
    "sns.set_context(\"paper\", font_scale = 2)\n",
    "\n",
    "sns.displot(data=results, x=results, kind=\"hist\", bins = 100, aspect = 1.5)\n",
    "\n",
    "\n",
    "# Print the results\n",
    "print(\"Mean result: \", mean_result)\n",
    "print(\"Standard deviation of result: \", std_dev_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8e9129",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T21:11:45.816703Z",
     "start_time": "2023-07-19T21:05:15.357021Z"
    }
   },
   "outputs": [],
   "source": [
    "for col in df_test.columns:\n",
    "    \n",
    "    print(\"col: \", col)\n",
    "    vals = list(df_test[col])\n",
    "    \n",
    "    f = Fitter(vals, distributions=get_distributions(),timeout=120, bins=50)#,xmin=0.1, xmax=1 ) \n",
    "    #get_common_distributions() ##['gamma','lognorm',\"beta\",\"burr\",\"norm\"]\n",
    "    f.fit()\n",
    "\n",
    "    f.summary()\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca53011",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T21:12:23.715024Z",
     "start_time": "2023-07-19T21:12:23.159415Z"
    }
   },
   "outputs": [],
   "source": [
    "df_test[\"age\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecccd6bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T21:14:40.515439Z",
     "start_time": "2023-07-19T21:14:39.643477Z"
    }
   },
   "outputs": [],
   "source": [
    "df_test[\"ind_risk\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a352b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5f9378",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b150e38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7fa2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_bounds = []\n",
    "\n",
    "# possible_values = {\n",
    "#     'age': [20, 86],\n",
    "#     'ind_risk': [0, 1],\n",
    "#     'income': [0, 150000],\n",
    "#     'pers_exp': [0, 6000],\n",
    "#     'house_exp': [0, 4000],\n",
    "#     'taxes': [0, 2500],\n",
    "#     'transp_telecom': [0, 2500],\n",
    "#     'hobby': [0, 3000],\n",
    "# }\n",
    "\n",
    "\n",
    "\n",
    "# for v in possible_values.values():\n",
    "    \n",
    "#     feature_bounds.append(tuple(v))\n",
    "\n",
    "\n",
    "# import numpy as np\n",
    "# from scipy.stats import norm\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# def generate_synthetic_data_with_bounds(correlation_matrix, num_samples, feature_bounds):\n",
    "#     num_features = correlation_matrix.shape[0]\n",
    "#     lower_bounds, upper_bounds = zip(*feature_bounds)\n",
    "#     scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    \n",
    "#     # Check if the correlation matrix is valid (symmetric and positive definite)\n",
    "#     if not np.allclose(correlation_matrix, correlation_matrix.T):\n",
    "#         raise ValueError(\"Correlation matrix must be symmetric.\")\n",
    "#     if not np.all(np.linalg.eigvals(correlation_matrix) > 0):\n",
    "#         raise ValueError(\"Correlation matrix must be positive definite.\")\n",
    "    \n",
    "#     # Generate synthetic data using multivariate normal distribution\n",
    "#     mean = np.zeros(num_features)\n",
    "#     synthetic_data = np.random.multivariate_normal(mean, correlation_matrix, num_samples)\n",
    "    \n",
    "#     # Apply Gaussian copula to maintain correlation structure\n",
    "#     synthetic_data = norm.cdf(synthetic_data)\n",
    "    \n",
    "#     # Scale the data to the specified bounds for each feature\n",
    "#     for i in range(num_features):\n",
    "#         synthetic_data[:, i] = lower_bounds[i] + synthetic_data[:, i] * (upper_bounds[i] - lower_bounds[i])\n",
    "    \n",
    "#     return synthetic_data\n",
    "\n",
    "# # Example usage:\n",
    "# correlation_matrix = np.array(l_corr_matrix)\n",
    "\n",
    "# num_samples = 250000\n",
    "\n",
    "# synthetic_data = generate_synthetic_data_with_bounds(correlation_matrix, num_samples, feature_bounds)\n",
    "# print(synthetic_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb0645c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_bounds = []\n",
    "\n",
    "# possible_values = {\n",
    "#     'age': [20, 86],\n",
    "#     'ind_risk': [0, 1],\n",
    "#     'income': [0, 150000],\n",
    "#     'pers_exp': [0, 6000],\n",
    "#     'house_exp': [0, 4000],\n",
    "#     'taxes': [0, 2500],\n",
    "#     'transp_telecom': [0, 2500],\n",
    "#     'hobby': [0, 3000],\n",
    "# }\n",
    "\n",
    "\n",
    "\n",
    "# for v in possible_values.values():\n",
    "    \n",
    "#     feature_bounds.append(tuple(v))\n",
    "\n",
    "\n",
    "# import numpy as np\n",
    "# from scipy.stats import norm\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# def generate_synthetic_data_with_bounds(correlation_matrix, num_samples, feature_bounds):\n",
    "#     num_features = correlation_matrix.shape[0]\n",
    "#     lower_bounds, upper_bounds = zip(*feature_bounds)\n",
    "#     scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    \n",
    "#     # Check if the correlation matrix is valid (symmetric and positive definite)\n",
    "#     if not np.allclose(correlation_matrix, correlation_matrix.T):\n",
    "#         raise ValueError(\"Correlation matrix must be symmetric.\")\n",
    "#     if not np.all(np.linalg.eigvals(correlation_matrix) > 0):\n",
    "#         raise ValueError(\"Correlation matrix must be positive definite.\")\n",
    "    \n",
    "#     # Generate synthetic data using multivariate normal distribution\n",
    "#     mean = np.zeros(num_features)\n",
    "#     synthetic_data = np.random.multivariate_normal(mean, correlation_matrix, num_samples)\n",
    "    \n",
    "#     # Apply Gaussian copula to maintain correlation structure\n",
    "#     synthetic_data = norm.cdf(synthetic_data)\n",
    "    \n",
    "#     # Scale the data to the specified bounds for each feature\n",
    "#     for i in range(num_features):\n",
    "#         synthetic_data[:, i] = lower_bounds[i] + synthetic_data[:, i] * (upper_bounds[i] - lower_bounds[i])\n",
    "    \n",
    "#     return synthetic_data\n",
    "\n",
    "# # Example usage:\n",
    "# correlation_matrix = np.array(l_corr_matrix)\n",
    "\n",
    "# num_samples = 250000\n",
    "\n",
    "# synthetic_data = generate_synthetic_data_with_bounds(correlation_matrix, num_samples, feature_bounds)\n",
    "# print(synthetic_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec94a906",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf7b78e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf079aa0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a689d71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3ea185",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T20:15:13.948425Z",
     "start_time": "2023-07-19T20:15:13.934869Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d5cb51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eba476d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4f9751",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d42ec764",
   "metadata": {},
   "source": [
    "# Old tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52d9f13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c1fa10d",
   "metadata": {},
   "source": [
    "## TODO: Personal expenses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbad1b7b",
   "metadata": {},
   "source": [
    "## Apply business rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ff83e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c49ec8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T08:19:10.973368Z",
     "start_time": "2023-07-19T08:19:10.920827Z"
    }
   },
   "outputs": [],
   "source": [
    "# test = f\"[(df_merged['marit_stat'] =='Married') & (df_merged['house_memb'] >'2')]\"\n",
    "test = f\"[(df_merged['age'] <25) & (df_merged['lv_educ'] !='Higher')]\"\n",
    "\n",
    "list_mask = eval(test)\n",
    "df_merged[list_mask[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fa4990",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T07:58:14.381436Z",
     "start_time": "2023-07-19T07:58:14.366410Z"
    }
   },
   "outputs": [],
   "source": [
    "df_merged.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de6038b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T08:00:05.427843Z",
     "start_time": "2023-07-19T08:00:05.353816Z"
    }
   },
   "outputs": [],
   "source": [
    "df_merged.loc[(df_merged['marit_stat'] =='Married') & (df_merged['house_memb'] >'2')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8018bbe",
   "metadata": {},
   "source": [
    "## Kolmogorov-Smirnov categorical tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4528591",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T19:15:26.616919Z",
     "start_time": "2023-07-19T19:15:26.601908Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats import ks_2samp\n",
    "\n",
    "old = {'sex': {'labels': ['M', 'F'], 'values': [0.4854368932, 0.5145631068]}}\n",
    "new = {'sex': {'labels': ['M', 'F'], 'values': [0.476, 0.524]}}\n",
    "\n",
    "# Extract the values for each category in the old and new distributions\n",
    "old_values = old['sex']['values']\n",
    "new_values = new['sex']['values']\n",
    "\n",
    "# Perform the KS test\n",
    "ks_statistic, p_value = ks_2samp(old_values, new_values)\n",
    "\n",
    "# Define the significance level (alpha) to test against the p-value\n",
    "alpha = 0.05\n",
    "\n",
    "# Check if the p-value is less than the significance level\n",
    "if p_value < alpha:\n",
    "    print(\"The new distribution is significantly different from the old distribution.\")\n",
    "else:\n",
    "    print(\"The new distribution is not significantly different from the old distribution.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadeff48",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T18:51:42.862646Z",
     "start_time": "2023-07-19T18:51:42.835796Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats import ks_2samp\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(12345678)\n",
    "x = np.random.normal(0, 1, 1000)\n",
    "y = np.random.normal(0, 1, 1000)\n",
    "z = np.random.normal(1.1, 0.9, 1000)\n",
    "\n",
    "b = np.array([1,2,3,4,5])\n",
    "c = np.array([1,2,3,4,5])\n",
    "d = np.array([50, 100, 200, 400, 800])\n",
    "\n",
    "a = 0.05\n",
    "\n",
    "res = ks_2samp(x, y)\n",
    "res2 = ks_2samp(b,c)\n",
    "res3 = ks_2samp(b,d)\n",
    "print(\"x,y ks: \", ks_2samp(x, y))\n",
    "# Ks_2sampResult(statistic=0.022999999999999909, pvalue=0.95189016804849647)\n",
    "print(\"x,z ks: \", ks_2samp(x, z))\n",
    "print(\"b,c ks: \", ks_2samp(b, c))\n",
    "print(\"b,d ks: \", ks_2samp(b, d))\n",
    "\n",
    "\n",
    "# Ks_2sampResult(statistic=0.41800000000000004, pvalue=3.7081494119242173e-77)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43884f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T18:51:44.022290Z",
     "start_time": "2023-07-19T18:51:44.008159Z"
    }
   },
   "outputs": [],
   "source": [
    "res3.pvalue <= a\n",
    "# False means Statistically insignificant - or the two distributions are considered the same!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd27a4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14e2e86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-ss2023_v2]",
   "language": "python",
   "name": "conda-env-.conda-ss2023_v2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "326.9px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "412.85px",
    "left": "1124.77px",
    "right": "20px",
    "top": "113px",
    "width": "530.467px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
